{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "from copy import deepcopy\n",
    "from math import sqrt, log\n",
    "from anytree import AnyNode\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sistemas de recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos 3 sistemas distintos de recompensa:\n",
    "- Recompensa simple por objetivos alcanzados.\n",
    "- Recompensa de disatncia.\n",
    "- Recompensa mixta (combinación de objetivos y distancias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward_simple(env):\n",
    "    \"\"\"\n",
    "    Calcula la recompensa basada únicamente en objetivos alcanzados y penalizaciones claras.\n",
    "\n",
    "    Args:\n",
    "        env (Environment): El entorno del robot (posiciones y estados).\n",
    "\n",
    "    Returns:\n",
    "        float: Recompensa acumulada según el estado actual.\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "\n",
    "    # Penalización por intentar recoger la pieza en una posición incorrecta.\n",
    "    if env.has_piece == 0 and env.action_last == 6 and (\n",
    "        env.robot_x_position != env.piece_x_position or\n",
    "        env.robot_y_position != env.piece_y_position or\n",
    "        env.robot_z_position != env.piece_z_position\n",
    "    ):\n",
    "        reward -= 50\n",
    "\n",
    "    # Recompensa por estar donde la pieza.\n",
    "    if env.robot_x_position == env.piece_x_position and env.robot_y_position == env.piece_y_position and env.robot_z_position == env.piece_z_position and env.has_piece == 0:\n",
    "        reward += 10\n",
    "\n",
    "    # Recompensa por recoger la pieza (solo una vez).\n",
    "    if env.has_piece == 1 and not hasattr(env, 'piece_collected'):\n",
    "        reward += 100\n",
    "        setattr(env, 'piece_collected', True)\n",
    "\n",
    "    # Penalización por abrir/cerrar pinza de forma repetitiva.\n",
    "    if not hasattr(env, 'pinza_toggle_count'):\n",
    "        env.pinza_toggle_count = 0\n",
    "\n",
    "    if env.action_last == 6:\n",
    "        env.pinza_toggle_count += 1\n",
    "        if env.pinza_toggle_count > 2: # Penalizar solo si ocurre más de dos veces\n",
    "            reward -= 500\n",
    "    else:\n",
    "        env.pinza_toggle_count = 0  # Reinicia si realiza otra acción\n",
    "\n",
    "    # Recompensa por progresar hacia el objetivo en los ejes.\n",
    "    if env.has_piece == 1:\n",
    "        aligned_axes = sum([\n",
    "            env.robot_x_position == env.goal_x_position,\n",
    "            env.robot_y_position == env.goal_y_position,\n",
    "            env.robot_z_position == env.goal_z_position\n",
    "        ])\n",
    "        reward += 50 * aligned_axes\n",
    "\n",
    "    # Recompensa por estar alineado donde el objetivo mientras lleva la pieza.\n",
    "    if env.has_piece == 1 and env.robot_x_position == env.goal_x_position and env.robot_y_position == env.goal_y_position and env.robot_z_position == env.goal_z_position:\n",
    "        reward += 500\n",
    "\n",
    "    # Recompensa por llevar la pieza al objetivo.\n",
    "    if (env.piece_x_position == env.goal_x_position and\n",
    "        env.piece_y_position == env.goal_y_position and\n",
    "        env.piece_z_position == env.goal_z_position and\n",
    "        not hasattr(env, 'goal_reached')):\n",
    "        reward += 500\n",
    "        setattr(env, 'goal_reached', True)\n",
    "\n",
    "    # Recompensa por soltar la pieza en el objetivo.\n",
    "    if env.has_piece == 0 and env.robot_x_position == env.goal_x_position and env.robot_y_position == env.goal_y_position and env.robot_z_position == env.goal_z_position:\n",
    "        reward += 5000\n",
    "\n",
    "    # Penalización por soltar la pieza fuera del objetivo.\n",
    "    if env.has_piece == 0 and not hasattr(env, 'piece_placed') and (\n",
    "        env.piece_x_position != env.goal_x_position or\n",
    "        env.piece_y_position != env.goal_y_position or\n",
    "        env.piece_z_position != env.goal_z_position\n",
    "    ):\n",
    "        reward -= 1000\n",
    "        setattr(env, 'piece_placed', True)\n",
    "\n",
    "    # Penalización adicional por soltar repetidamente la pieza en posiciones incorrectas.\n",
    "    if not hasattr(env, 'incorrect_drop_count'):\n",
    "        env.incorrect_drop_count = 0\n",
    "\n",
    "    if env.has_piece == 0 and (\n",
    "        env.piece_x_position != env.goal_x_position or\n",
    "        env.piece_y_position != env.goal_y_position or\n",
    "        env.piece_z_position != env.goal_z_position\n",
    "    ):\n",
    "        env.incorrect_drop_count += 1\n",
    "        if env.incorrect_drop_count > 1:  # Penalizar solo si ocurre más de una vez\n",
    "            reward -= 500 * env.incorrect_drop_count\n",
    "    else:\n",
    "        env.incorrect_drop_count = 0  # Reiniciar el contador si la pieza está en el lugar correcto\n",
    "\n",
    "    # Recompensa constante por mantener la pieza.\n",
    "    if env.has_piece == 1 and not (env.robot_x_position == env.goal_x_position and env.robot_y_position == env.goal_y_position and env.robot_z_position == env.goal_z_position):\n",
    "        reward += 50\n",
    "\n",
    "    # Penalización por movimientos redundantes.\n",
    "    if hasattr(env, 'visited_positions'):\n",
    "        current_position = (env.robot_x_position, env.robot_y_position, env.robot_z_position)\n",
    "        if current_position in env.visited_positions:\n",
    "            reward -= 50\n",
    "        else:\n",
    "            env.visited_positions.add(current_position)\n",
    "    else:\n",
    "        env.visited_positions = set([(env.robot_x_position, env.robot_y_position, env.robot_z_position)])\n",
    "\n",
    "    # Penalización por entrar en bucles.\n",
    "    if hasattr(env, 'recent_positions'):\n",
    "        current_position = (env.robot_x_position, env.robot_y_position, env.robot_z_position)\n",
    "        env.recent_positions.append(current_position)\n",
    "        # Limitar el historial a las últimas 10 posiciones.\n",
    "        if len(env.recent_positions) > 10:\n",
    "            env.recent_positions.pop(0)\n",
    "        # Penalizar bucles.\n",
    "        if len(env.recent_positions) > len(set(env.recent_positions)):\n",
    "            reward -= 200\n",
    "    else:\n",
    "        env.recent_positions = [(env.robot_x_position, env.robot_y_position, env.robot_z_position)]\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward_distance(env):\n",
    "    \"\"\"\n",
    "    Calcula la recompensa únicamente en función de las distancias al objetivo y la pieza.\n",
    "\n",
    "    Args:\n",
    "        env (Environment): El entorno que contiene el estado actual del robot. \n",
    "\n",
    "    Returns:\n",
    "        float: La recompensa calculada para el estado actual del entorno.\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "\n",
    "    # Calcular la distancia al objeto\n",
    "    distance_to_piece = ((env.robot_x_position - env.piece_x_position) ** 2 +\n",
    "                         (env.robot_y_position - env.piece_y_position) ** 2 +\n",
    "                         (env.robot_z_position - env.piece_z_position) ** 2) ** 0.5\n",
    "\n",
    "    # Calcular la distancia al objetivo\n",
    "    distance_to_goal = ((env.robot_x_position - env.goal_x_position) ** 2 +\n",
    "                        (env.robot_y_position - env.goal_y_position) ** 2 +\n",
    "                        (env.robot_z_position - env.goal_z_position) ** 2) ** 0.5\n",
    "\n",
    "    if env.has_piece == 0:\n",
    "        # Recompensa basada en la cercanía al objeto\n",
    "        reward += max(0, 100 - distance_to_piece * 10)\n",
    "    else:\n",
    "        # Recompensa basada en la cercanía al objetivo\n",
    "        reward += max(0, 500 - distance_to_goal * 10)\n",
    "\n",
    "        # Recompensa adicional por reducir la distancia al objetivo\n",
    "        if hasattr(env, 'previous_distance_to_goal'):\n",
    "            if distance_to_goal < env.previous_distance_to_goal:\n",
    "                reward += 50\n",
    "        env.previous_distance_to_goal = distance_to_goal\n",
    "\n",
    "    piece_distance_to_goal = ((env.piece_x_position - env.goal_x_position) ** 2 +\n",
    "                            (env.piece_y_position - env.goal_y_position) ** 2 +\n",
    "                            (env.piece_z_position - env.goal_z_position) ** 2) ** 0.5\n",
    "\n",
    "    # Recompensa por llevar la pieza al objetivo\n",
    "    if piece_distance_to_goal == 0 and env.has_piece == 0:\n",
    "        reward += 100000\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_reward_mixed(env):\n",
    "    \"\"\"\n",
    "    Calcula la recompensa mixta basada en objetivos alcanzados y distancia al objetivo. Añade penalizaciones por movimientos redundantes y bucles.\n",
    "\n",
    "    Args:\n",
    "        env (Environment): El entorno que contiene el estado actual del robot.\n",
    "\n",
    "    Returns:\n",
    "        float: La recompensa calculada para el estado actual del entorno.\n",
    "    \"\"\"\n",
    "    reward = 0 \n",
    "\n",
    "    if env.has_piece == 0:\n",
    "        # Recompensa proporcional a la cercanía al objeto.\n",
    "        distance_to_piece = ((env.robot_x_position - env.piece_x_position) ** 2 +\n",
    "                             (env.robot_y_position - env.piece_y_position) ** 2 +\n",
    "                             (env.robot_z_position - env.piece_z_position) ** 2) ** 0.5\n",
    "        reward += max(0, 50 - distance_to_piece * 2)\n",
    "    else:\n",
    "        # Recompensa proporcional a la cercanía al objetivo.\n",
    "        distance_to_goal = ((env.robot_x_position - env.goal_x_position) ** 2 +\n",
    "                            (env.robot_y_position - env.goal_y_position) ** 2 +\n",
    "                            (env.robot_z_position - env.goal_z_position) ** 2) ** 0.5\n",
    "        reward += max(0, 200 - distance_to_goal * 2)\n",
    "\n",
    "        # Penalización y recompensa acumulativa basada en la distancia al objetivo.\n",
    "        if hasattr(env, 'previous_distance_to_goal'):\n",
    "            if distance_to_goal > env.previous_distance_to_goal:\n",
    "                reward -= 5000\n",
    "            else:\n",
    "                reward += 10000\n",
    "        env.previous_distance_to_goal = distance_to_goal\n",
    "\n",
    "    # Recompensa por llegar a la pieza.\n",
    "    if env.piece_x_position == env.robot_x_position and env.piece_y_position == env.robot_y_position and env.piece_z_position == env.robot_z_position:\n",
    "        reward += 5000\n",
    "\n",
    "    # Recompensa por recoger la pieza (solo una vez).\n",
    "    if env.has_piece == 1 and not hasattr(env, 'piece_collected'):\n",
    "        reward += 100\n",
    "        setattr(env, 'piece_collected', True)\n",
    "\n",
    "    # Recompensa constante por mantener la pieza.\n",
    "    if env.has_piece == 1:\n",
    "        reward += 10000\n",
    "\n",
    "    # Recompensa por llegar al objetivo con la pieza.\n",
    "    if env.piece_x_position == env.robot_x_position == env.goal_x_position and env.piece_y_position == env.robot_y_position == env.goal_y_position and env.piece_z_position == env.robot_z_position == env.goal_z_position:\n",
    "        reward += 500000\n",
    "\n",
    "    # Recompensa por alcanzar el objetivo.\n",
    "    if (env.piece_x_position == env.goal_x_position and\n",
    "        env.piece_y_position == env.goal_y_position and\n",
    "        env.piece_z_position == env.goal_z_position and\n",
    "        not hasattr(env, 'goal_reached')):\n",
    "        reward += 1000000\n",
    "        setattr(env, 'goal_reached', True)\n",
    "\n",
    "    # Penalización por soltar la pieza en una posición incorrecta.\n",
    "    if env.has_piece == 0 and not hasattr(env, 'piece_placed') and (\n",
    "        env.piece_x_position != env.goal_x_position or\n",
    "        env.piece_y_position != env.goal_y_position or\n",
    "        env.piece_z_position != env.goal_z_position\n",
    "    ):\n",
    "        reward -= 10000\n",
    "        setattr(env, 'piece_placed', True)\n",
    "\n",
    "    # Penalización por movimientos redundantes.\n",
    "    if hasattr(env, 'visited_positions'):\n",
    "        current_position = (env.robot_x_position, env.robot_y_position, env.robot_z_position)\n",
    "        if current_position in env.visited_positions:\n",
    "            reward -= 50\n",
    "        else:\n",
    "            env.visited_positions.add(current_position)\n",
    "    else:\n",
    "        env.visited_positions = set([(env.robot_x_position, env.robot_y_position, env.robot_z_position)])\n",
    "\n",
    "    # Penalización por entrar en bucles.\n",
    "    if hasattr(env, 'recent_positions'):\n",
    "        current_position = (env.robot_x_position, env.robot_y_position, env.robot_z_position)\n",
    "        env.recent_positions.append(current_position)\n",
    "        # Limitar el historial a las últimas 10 posiciones.\n",
    "        if len(env.recent_positions) > 10:\n",
    "            env.recent_positions.pop(0)\n",
    "        # Penaliar bucles.\n",
    "        if len(env.recent_positions) > len(set(env.recent_positions)):\n",
    "            reward -= 20000\n",
    "    else:\n",
    "        env.recent_positions = [(env.robot_x_position, env.robot_y_position, env.robot_z_position)]\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotPickAndPlaceEnv(gymnasium.Env):\n",
    "    \"\"\"\n",
    "    Representa el entorno del robot para resolver el problema de pick & place.\n",
    "\n",
    "    Este entorno simula un espacio discreto en 3D donde un robot debe recoger una \n",
    "    pieza en un punto inicial y transportarla a un objetivo. El robot puede moverse \n",
    "    en las direcciones X, Y y Z, además de abrir o cerrar su pinza para coger y soltar la pieza.\n",
    "\n",
    "    Attributes:\n",
    "        robot_position: Posición actual del robot en (x, y, z).\n",
    "        piece_position: Posición actual de la pieza en (x, y, z). (-1, -1, -1 si está siendo transportada).\n",
    "        goal_position: Posición del objetivo en (x, y, z).\n",
    "        has_piece (bool): Indica si el robot tiene la pieza agarrada (1 si tiene la pieza, 0 en caso contrario).\n",
    "        step_limit (int): Número máximo de pasos permitidos.\n",
    "        action_space (gymnasium.spaces.Discrete): Espacio de acciones (0-6).\n",
    "        observation_space (gymnasium.spaces.Tuple): Espacio de observaciones del entorno.\n",
    "        reward_function: Función de recompensa utilizada para evaluar los pasos del robot.\n",
    "    \"\"\"\n",
    "    def __init__(self, reward_function=calculate_reward_simple):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno del robot y establece la función de recompensa, por defecto simple.\n",
    "\n",
    "        Configura los espacios de acción y observación, y define las posiciones iniciales \n",
    "        del robot, la pieza y el objetivo.\n",
    "        \"\"\"\n",
    "        super(RobotPickAndPlaceEnv, self).__init__()\n",
    "\n",
    "        # Definir espacio de acciones: 0 = mover izquierda, 1 = mover derecha, 2 = mover abajo, 3 = mover arriba, 4 = mover atrás, 5 = mover delante, 6 = abrir/cerrar pinza\n",
    "        self.action_space = spaces.Discrete(7)\n",
    "\n",
    "        # Definir espacio de observaciones\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(10),  # Posición del robot en el eje X (0-9)\n",
    "            spaces.Discrete(10),  # Posición del robot en el eje Y (0-9)\n",
    "            spaces.Discrete(10),  # Posición del robot en el eje Z (0-9)\n",
    "            spaces.Discrete(2),   # Tiene pieza (0 o 1)\n",
    "            spaces.Discrete(11),  # Posición de la pieza en el eje X (0-9) (-1 si está siendo transportada)\n",
    "            spaces.Discrete(11),  # Posición de la pieza en el eje Y (0-9) (-1 si está siendo transportada)\n",
    "            spaces.Discrete(11)   # Posición de la pieza en el eje Z (0-9) (-1 si está siendo transportada)\n",
    "        ))\n",
    "\n",
    "        # Establecer función de recompensa\n",
    "        self.reward_function = reward_function\n",
    "        \n",
    "        # Inicializar estado\n",
    "        self.reset()\n",
    "\n",
    "        # Inicializar figura, eje y gráfica de matplotlib\n",
    "        self.fig = None\n",
    "        self.ax = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reinicia el entorno a su estado inicial.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Observación inicial del entorno.\n",
    "        \"\"\"\n",
    "        # Condiciones iniciales\n",
    "        self.robot_x_position = 0  # El robot comienza en la posición (0,0,0)\n",
    "        self.robot_y_position = 0\n",
    "        self.robot_z_position = 0\n",
    "        self.has_piece = 0         # El robot no tiene la pieza\n",
    "        self.piece_x_position = 5  # La pieza comienza en la posición (5,5,5)\n",
    "        self.piece_y_position = 5\n",
    "        self.piece_z_position = 5\n",
    "        self.goal_x_position = 9   # El objetivo está en la posición (9, 7, 9)\n",
    "        self.goal_y_position = 7\n",
    "        self.goal_z_position = 9\n",
    "        self.steps = 0             # Contador de pasos por episodio\n",
    "\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Realiza un paso en el entorno según la acción proporcionada.\n",
    "\n",
    "        Args:\n",
    "            action (int): Índice de la acción a realizar (0-6).\n",
    "\n",
    "        Returns:\n",
    "            tuple: Estado actual del entorno, recompensa obtenida, \n",
    "                indicador de si la tarea ha terminado, indicador de truncado, \n",
    "                y diccionario de información adicional.\n",
    "        \"\"\"\n",
    "        # Asegurarse de que la acción es válida\n",
    "        assert self.action_space.contains(action), \"Acción inválida\"\n",
    "        done = False\n",
    "        truncated = False\n",
    "        self.action_last = action\n",
    "\n",
    "        # Realizar movimientos según la acción\n",
    "        if action == 0:  # Mover izquierda\n",
    "            self.robot_x_position = max(0, self.robot_x_position - 1)\n",
    "        elif action == 1:  # Mover derecha\n",
    "            self.robot_x_position = min(9, self.robot_x_position + 1)\n",
    "        elif action == 2:  # Mover abajo\n",
    "            self.robot_y_position = max(0, self.robot_y_position - 1)\n",
    "        elif action == 3:  # Mover arriba\n",
    "            self.robot_y_position = min(9, self.robot_y_position + 1)\n",
    "        elif action == 4:  # Mover atrás\n",
    "            self.robot_z_position = max(0, self.robot_z_position - 1)\n",
    "        elif action == 5:  # Mover adelante\n",
    "            self.robot_z_position = min(9, self.robot_z_position + 1)\n",
    "        elif action == 6:  # Abrir/cerrar pinza\n",
    "            if not self.has_piece:\n",
    "                if (self.robot_x_position == self.piece_x_position and\n",
    "                    self.robot_y_position == self.piece_y_position and\n",
    "                    self.robot_z_position == self.piece_z_position):\n",
    "                    self.has_piece = 1\n",
    "                    self.piece_x_position = -1\n",
    "                    self.piece_y_position = -1\n",
    "                    self.piece_z_position = -1\n",
    "            else:\n",
    "                self.has_piece = 0\n",
    "                self.piece_x_position = self.robot_x_position\n",
    "                self.piece_y_position = self.robot_y_position\n",
    "                self.piece_z_position = self.robot_z_position\n",
    "                if (self.piece_x_position == self.goal_x_position and\n",
    "                    self.piece_y_position == self.goal_y_position and\n",
    "                    self.piece_z_position == self.goal_z_position):\n",
    "                    done = True\n",
    "\n",
    "        # Calcular recompensa y verificar si se ha alcanzado el objetivo\n",
    "        reward = self.reward_function(self)\n",
    "        self.steps += 1\n",
    "        \n",
    "        if self.steps >= 1000:  # Límite de pasos por episodio\n",
    "            truncated = True\n",
    "        return self._get_observation(), reward, done, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renderiza el estado actual del entorno en una gráfica 3D.\n",
    "        \"\"\"\n",
    "        # Inicializar figura y ejes si no existen\n",
    "        if self.fig is None or self.ax is None:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "            # Inicializar gráficos del robot, la pieza y el objetivo\n",
    "            self.robot_plot, = self.ax.plot([], [], [], \"go\", label=\"Robot\", markersize=7)  # Punto verde para el robot\n",
    "            self.goal_plot, = self.ax.plot([], [], [], \"ro\", label=\"Objetivo\", markersize=8)  # Punto rojo para el objetivo\n",
    "            self.piece_plot, = self.ax.plot([], [], [], \"bo\", label=\"Pieza\", markersize=6)  # Punto azul para la pieza\n",
    "\n",
    "            # Configuración de los límites del eje (ajústalos según tu entorno)\n",
    "            self.ax.set_xlim([0, 10])\n",
    "            self.ax.set_ylim([0, 10])\n",
    "            self.ax.set_zlim([0, 10])\n",
    "\n",
    "            # Etiquetas y leyenda\n",
    "            self.ax.set_xlabel(\"X\")\n",
    "            self.ax.set_ylabel(\"Y\")\n",
    "            self.ax.set_zlabel(\"Z\")\n",
    "            self.ax.legend()\n",
    "\n",
    "        # Actualizar posición del robot\n",
    "        self.robot_plot.set_data([self.robot_x_position], [self.robot_y_position])\n",
    "        self.robot_plot.set_3d_properties([self.robot_z_position])\n",
    "\n",
    "        # Actualizar posición de la pieza\n",
    "        if self.has_piece:\n",
    "            self.piece_plot.set_data([self.robot_x_position], [self.robot_y_position])\n",
    "            self.piece_plot.set_3d_properties([self.robot_z_position])\n",
    "        else:\n",
    "            self.piece_plot.set_data([self.piece_x_position], [self.piece_y_position])\n",
    "            self.piece_plot.set_3d_properties([self.piece_z_position])\n",
    "\n",
    "        # Actualizar posición del objetivo\n",
    "        self.goal_plot.set_data([self.goal_x_position], [self.goal_y_position])\n",
    "        self.goal_plot.set_3d_properties([self.goal_z_position])\n",
    "\n",
    "        # Actualizar el título con el número de pasos\n",
    "        self.ax.set_title(f\"Paso: {self.steps} | Posición Robot: ({self.robot_x_position}, {self.robot_y_position}, {self.robot_z_position}) | Posición Pieza: ({self.piece_x_position}, {self.piece_y_position}, {self.piece_z_position})\")\n",
    "\n",
    "        # Dibujar y pausar para visualización en tiempo real\n",
    "        plt.draw()\n",
    "        plt.pause(0.01)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Obtiene la observación actual del entorno.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Estado actual del entorno.\n",
    "        \"\"\"\n",
    "        return (self.robot_x_position, self.robot_y_position, self.robot_z_position, self.has_piece, self.piece_x_position, self.piece_y_position, self.piece_z_position)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Cierra la visualización del entorno.\n",
    "        \"\"\"\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemetación de Monte Carlo Tree Search (MCTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase MCTSNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode(AnyNode):\n",
    "    \"\"\"\n",
    "    Nodo para MCTS que hereda de AnyNode.\n",
    "    La relación padre-hijo es gestionada por AnyTree:\n",
    "      - self.parent = nodo padre\n",
    "      - self.children = conjunto de hijos\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        state,\n",
    "        parent=None,\n",
    "        action_from_parent=None,\n",
    "        reward_from_parent=0.0,\n",
    "        done=False,\n",
    "        c_value=1.4\n",
    "    ):\n",
    "        super().__init__(parent=parent)\n",
    "        self.env = env\n",
    "        self.state = state\n",
    "        self.action_from_parent = action_from_parent\n",
    "        self.reward_from_parent = reward_from_parent  # <--- Guardar la recompensa recibida al llegar a este nodo\n",
    "        self.done = done\n",
    "        self.c_value = c_value\n",
    "\n",
    "        # MCTS stats\n",
    "        self.Q = 0.0\n",
    "        self.N = 0\n",
    "\n",
    "        # Para expandir acciones\n",
    "        self.remaining_actions = list(range(env.action_space.n))\n",
    "        self.fully_expanded = False\n",
    "\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Un nodo se considera 'hoja' si está en estado terminal (done),\n",
    "        o bien si no tiene hijos después de haber sido visitado alguna vez.\n",
    "        (es decir, no se ha expandido o ya se expandió y no hay más acciones).\n",
    "        \"\"\"\n",
    "        return self.done or (len(self.children) == 0 and self.N > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    \"\"\"\n",
    "    Clase principal de MCTS que construye un árbol usando AnyNode.\n",
    "    Funciona en cualquier entorno Gym (Discrete) siempre que podamos clonar su estado.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_env, c_value=1.4, max_depth=50, rollout_policy=\"random\"):\n",
    "        \"\"\"\n",
    "        root_env: entorno Gym (o Gymnasium).\n",
    "                  Se usará para inicializar el nodo raíz.\n",
    "        c_value: constante de exploración UCB.\n",
    "        max_depth: profundidad máxima del rollout en simulate().\n",
    "        rollout_policy: define la estrategia de simulación (\"random\", \"heuristic\", etc.).\n",
    "        \"\"\"\n",
    "        # Reiniciamos el entorno raíz para obtener la observación inicial\n",
    "        obs = root_env.reset()\n",
    "\n",
    "        # Creamos el nodo raíz con un clon de root_env\n",
    "        self.root = MCTSNode(\n",
    "            env=deepcopy(root_env),\n",
    "            state=obs,\n",
    "            parent=None,\n",
    "            done=False,\n",
    "            c_value=c_value\n",
    "        )\n",
    "        self.c_value = c_value\n",
    "        self.max_depth = max_depth\n",
    "        self.rollout_policy = rollout_policy\n",
    "\n",
    "    def optimize(self, n_iter=1000):\n",
    "        \"\"\"\n",
    "        Ejecuta el bucle de MCTS n_iter veces (each iteration = select -> expand -> simulate -> backprop).\n",
    "        \"\"\"\n",
    "        for _ in range(n_iter):\n",
    "            # 1) Selección\n",
    "            node = self.select(self.root)\n",
    "            # 2) Expansión\n",
    "            leaf = self.expand(node)\n",
    "            # 3) Simulación\n",
    "            value = self.simulate(leaf)\n",
    "            # 4) Retropropagación\n",
    "            self.backpropagate(leaf, value)\n",
    "\n",
    "    def select(self, node):\n",
    "        \"\"\"\n",
    "        Navega el árbol desde 'node' eligiendo hijos vía UCB1 hasta encontrar\n",
    "        un nodo que no esté totalmente expandido o sea hoja.\n",
    "        \"\"\"\n",
    "        while not node.done and node.fully_expanded and len(node.children) > 0:\n",
    "            node = self._best_child_ucb1(node)\n",
    "        return node\n",
    "\n",
    "    def _best_child_ucb1(self, node):\n",
    "        \"\"\"\n",
    "        Dado un nodo, retorna el hijo con mayor valor UCB1.\n",
    "        UCB1 = (Q / N) + c * sqrt(log(N_parent) / N_child)\n",
    "        \"\"\"\n",
    "        best_value = float(\"-inf\")\n",
    "        best_child = None\n",
    "\n",
    "        for child in node.children:\n",
    "            if child.N == 0:\n",
    "                # Evitar división por cero\n",
    "                ucb = float(\"inf\")\n",
    "            else:\n",
    "                exploit = child.Q / child.N\n",
    "                explore = self.c_value * sqrt(log(node.N) / child.N)\n",
    "                ucb = exploit + explore\n",
    "\n",
    "            if ucb > best_value:\n",
    "                best_value = ucb\n",
    "                best_child = child\n",
    "\n",
    "        return best_child\n",
    "\n",
    "    def expand(self, node):\n",
    "        if node.done:\n",
    "            return node\n",
    "\n",
    "        actions = node.remaining_actions\n",
    "        if len(actions) == 0:\n",
    "            return node\n",
    "\n",
    "        if self.rollout_policy == \"heuristic\":\n",
    "            action = heuristic_policy(node.env)\n",
    "            if action not in actions:\n",
    "                action = np.random.choice(actions)\n",
    "        else:\n",
    "            action = np.random.choice(actions)\n",
    "\n",
    "        actions.remove(action)\n",
    "        if len(actions) == 0:\n",
    "            node.fully_expanded = True\n",
    "\n",
    "        env_copy = deepcopy(node.env)\n",
    "        obs, reward, done, truncated, info = env_copy.step(action)\n",
    "\n",
    "        child_node = MCTSNode(\n",
    "            env=env_copy,\n",
    "            state=obs,\n",
    "            parent=node,\n",
    "            action_from_parent=action,\n",
    "            reward_from_parent=reward,\n",
    "            done=(done or truncated),\n",
    "            c_value=node.c_value\n",
    "        )\n",
    "        return child_node\n",
    "\n",
    "    def simulate(self, node):\n",
    "        \"\"\"\n",
    "        Realiza un rollout a partir de 'node' hasta 'max_depth' o estado terminal.\n",
    "        Retorna la suma de recompensas.\n",
    "        \"\"\"\n",
    "        if node.done:\n",
    "            return 0.0\n",
    "\n",
    "        simulation_env = deepcopy(node.env)\n",
    "        total_reward = 0.0\n",
    "        depth = 0\n",
    "\n",
    "        while depth < self.max_depth:\n",
    "            # Elige acción según la política de rollout\n",
    "            action = self._rollout_policy(simulation_env)\n",
    "            obs, reward, done, truncated, info = simulation_env.step(action)\n",
    "            total_reward += reward\n",
    "            depth += 1\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    def _rollout_policy(self, env):\n",
    "        \"\"\"\n",
    "        Define la política usada en la fase de simulación/rollout.\n",
    "        \"\"\"\n",
    "        if self.rollout_policy == \"random\":\n",
    "            return env.action_space.sample()\n",
    "        elif self.rollout_policy == \"heuristic\":\n",
    "            return heuristic_policy(env)\n",
    "        else:\n",
    "            # Por defecto, random si no se reconoce la política\n",
    "            return env.action_space.sample()\n",
    "\n",
    "    def backpropagate(self, node, value):\n",
    "        \"\"\"\n",
    "        Propaga el valor de la simulación hacia la raíz,\n",
    "        actualizando Q y N en cada nodo del camino.\n",
    "        \"\"\"\n",
    "        current = node\n",
    "        while current is not None:\n",
    "            current.N += 1\n",
    "            current.Q += (current.reward_from_parent + value)\n",
    "            current = current.parent\n",
    "\n",
    "    def best_action(self):\n",
    "        \"\"\"\n",
    "        Retorna la acción del hijo de la raíz que tenga más visitas N.\n",
    "        \"\"\"\n",
    "        if len(self.root.children) == 0:\n",
    "            # Si no hay hijos, devolvemos una acción aleatoria\n",
    "            return np.random.choice(self.root.env.action_space.n)\n",
    "        best_child = max(self.root.children, key=lambda c: c.N)\n",
    "        return best_child.action_from_parent\n",
    "\n",
    "    def re_root(self, action):\n",
    "        \"\"\"\n",
    "        Hace 're-rooting' de la raíz al hijo que surge de 'action'.\n",
    "        Si no existe el hijo, no hace nada.\n",
    "        \"\"\"\n",
    "        matching_children = [c for c in self.root.children if c.action_from_parent == action]\n",
    "        if not matching_children:\n",
    "            return\n",
    "        new_root = matching_children[0]\n",
    "        new_root.parent = None  # AnyTree: quitamos la relación con la antigua raíz\n",
    "        self.root = new_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heurística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_policy(env):\n",
    "    rx, ry, rz, has_piece, px, py, pz = (\n",
    "        env.robot_x_position, env.robot_y_position, env.robot_z_position,\n",
    "        env.has_piece, env.piece_x_position, env.piece_y_position, env.piece_z_position\n",
    "    )\n",
    "    gx, gy, gz = env.goal_x_position, env.goal_y_position, env.goal_z_position\n",
    "\n",
    "    if has_piece == 0:\n",
    "        # Si está en la pieza, recogerla\n",
    "        if (rx, ry, rz) == (px, py, pz):\n",
    "            return 6  # Cerrar pinza\n",
    "        \n",
    "        # Priorizar el movimiento hacia el eje con mayor distancia\n",
    "        distances = [\n",
    "            (abs(rx - px), 1 if rx < px else 0),\n",
    "            (abs(ry - py), 3 if ry < py else 2),\n",
    "            (abs(rz - pz), 5 if rz < pz else 4),\n",
    "        ]\n",
    "        return max(distances, key=lambda x: x[0])[1]\n",
    "\n",
    "    else:\n",
    "        # Si está en el objetivo, soltar la pieza\n",
    "        if (rx, ry, rz) == (gx, gy, gz):\n",
    "            return 6  # Soltar pinza\n",
    "        \n",
    "        # Priorizar el movimiento hacia el eje con mayor distancia al objetivo\n",
    "        distances = [\n",
    "            (abs(rx - gx), 1 if rx < gx else 0),\n",
    "            (abs(ry - gy), 3 if ry < gy else 2),\n",
    "            (abs(rz - gz), 5 if rz < gz else 4),\n",
    "        ]\n",
    "        return max(distances, key=lambda x: x[0])[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot Pick and Place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renderizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcts_robot(env, c_value=1.4, n_iter_mcts=1000, rollout_policy=\"random\", max_steps=100, max_depth=150):\n",
    "    \"\"\"\n",
    "    Ejecuta MCTS para un episodio en RobotPickAndPlaceEnv.\n",
    "    \"\"\"\n",
    "    obs = env.reset()  # O (obs, info) si tu reset lo maneja así\n",
    "    mcts = MCTS(root_env=env, c_value=c_value, max_depth=max_depth, rollout_policy=rollout_policy)\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    step_count = 0\n",
    "\n",
    "    while not done and step_count < max_steps:\n",
    "        # 1) MCTS\n",
    "        mcts.optimize(n_iter=n_iter_mcts)\n",
    "\n",
    "        # 2) Seleccionar la mejor acción\n",
    "        action = mcts.best_action()\n",
    "\n",
    "        # 3) Paso real\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        #print(f\"Step={step_count}, Action={action}, State={obs}, Reward={reward}, Done={done}\")\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "\n",
    "        # 4) re-root\n",
    "        mcts.re_root(action)\n",
    "\n",
    "        # Render si quieres ver la evolución 3D (comentado para no abrir ventanas en cada paso)\n",
    "        env.render()\n",
    "\n",
    "    return total_reward, step_count, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_study_robot(reward_function=calculate_reward_simple):\n",
    "    c_values = [0.5, 1.4, 2.0]\n",
    "    n_iters = [1000, 3000]\n",
    "    policies = [\"random\", \"heuristic\"]\n",
    "    n_runs = 3\n",
    "\n",
    "    results = []\n",
    "    for c_val in c_values:\n",
    "        for n_it in n_iters:\n",
    "            for pol in policies:\n",
    "                rews = []\n",
    "                steps_list = []\n",
    "                for _ in range(n_runs):\n",
    "                    env = RobotPickAndPlaceEnv(reward_function=reward_function)\n",
    "                    r, s, done = run_mcts_robot(env, c_value=c_val,\n",
    "                                          n_iter_mcts=n_it,\n",
    "                                          rollout_policy=pol,\n",
    "                                          max_steps=150)\n",
    "                    rews.append(r)\n",
    "                    steps_list.append(s)\n",
    "                    if done:\n",
    "                        break\n",
    "                avg_r = np.mean(rews)\n",
    "                avg_s = np.mean(steps_list)\n",
    "                results.append((c_val, n_it, pol, avg_r, avg_s))\n",
    "                print(f\"Sensibilidad Robot -> c={c_val}, n={n_it}, pol={pol} -> reward={avg_r:.1f}, steps={avg_s:.1f}, done={done}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def sensitivity_study_robot_single_run(reward_function=calculate_reward_simple):\n",
    "    c_values = [0.5, 1.4, 2.0]\n",
    "    n_iters = [1000, 3000]\n",
    "    policies = [\"random\", \"heuristic\"]\n",
    "\n",
    "    results = []\n",
    "    for c_val in c_values:\n",
    "        for n_it in n_iters:\n",
    "            for pol in policies:\n",
    "                env = RobotPickAndPlaceEnv(reward_function=reward_function)\n",
    "                r, s, done = run_mcts_robot(env, c_value=c_val,\n",
    "                                            n_iter_mcts=n_it,\n",
    "                                            rollout_policy=pol,\n",
    "                                            max_steps=100)\n",
    "                results.append((c_val, n_it, pol, r, s, done))\n",
    "                print(f\"Sensibilidad Robot -> c={c_val}, n={n_it}, pol={pol} -> reward={r:.1f}, steps={s:.1f}, done={done}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_sensitivity_results(results):\n",
    "    \"\"\"\n",
    "    Genera una gráfica que muestra el impacto de los parámetros `c`, `n` y `p` en los resultados,\n",
    "    incluyendo marcadores para indicar cuándo se alcanzó `done=True`.\n",
    "    \n",
    "    Args:\n",
    "        results (list): Lista de tuplas (c, n, p, avg_r, avg_s, done).\n",
    "    \"\"\"\n",
    "    policies = list(set([p for _, _, p, _, _, _ in results]))\n",
    "    iterations = list(set([n for _, n, _, _, _, _ in results]))\n",
    "    c_values = sorted(set([c for c, _, _, _, _, _ in results]))\n",
    "    \n",
    "    fig, axs = plt.subplots(len(policies), len(iterations), figsize=(15, 10), sharex=True, sharey=True)\n",
    "    \n",
    "    for i, pol in enumerate(policies):\n",
    "        for j, n in enumerate(iterations):\n",
    "            ax = axs[i, j]\n",
    "            filtered_results = [(c, avg_r, done) for c, n_iter, p, avg_r, _, done in results if p == pol and n_iter == n]\n",
    "            c_vals, avg_rewards, dones = zip(*filtered_results)\n",
    "            \n",
    "            ax.plot(c_vals, avg_rewards, label=f'n={n}, {pol}')\n",
    "            done_indices = [k for k, d in enumerate(dones) if d]\n",
    "            if done_indices:\n",
    "                ax.scatter(\n",
    "                    [c_vals[k] for k in done_indices],\n",
    "                    [avg_rewards[k] for k in done_indices],\n",
    "                    color=\"black\", label=\"Done\"\n",
    "                )\n",
    "            ax.set_title(f'Pol: {pol}, n: {n}')\n",
    "            ax.set_xlabel(\"c\")\n",
    "            ax.set_ylabel(\"Reward\")\n",
    "            ax.legend()\n",
    "            ax.grid()\n",
    "    \n",
    "    fig.suptitle(\"Impacto de la constante c en el robot para distintas políticas e iteraciones\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sensitivity_study_robot_single_run(reward_function=calculate_reward_mixed)\n",
    "# results = sensitivity_study_robot(reward_function=calculate_reward_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_sensitivity_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renderizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcts_frozenlake(is_slippery=False, c_value=1.4, n_iter_mcts=1000, rollout_policy=\"random\"):\n",
    "    env = gymnasium.make('FrozenLake-v1', is_slippery=is_slippery, render_mode=None)\n",
    "    obs, _info = env.reset()\n",
    "\n",
    "    # Creamos MCTS con el nodo raíz\n",
    "    mcts = MCTS(\n",
    "        root_env=env,\n",
    "        c_value=c_value,\n",
    "        max_depth=50,\n",
    "        rollout_policy=rollout_policy\n",
    "    )\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    max_steps = 100\n",
    "\n",
    "    while not done and step_count < max_steps:\n",
    "        # 1) Ejecutar MCTS\n",
    "        mcts.optimize(n_iter=n_iter_mcts)\n",
    "\n",
    "        # 2) Seleccionar la mejor acción\n",
    "        action = mcts.best_action()\n",
    "\n",
    "        # 3) Paso en el entorno real\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "\n",
    "        # 4) Re-root\n",
    "        mcts.re_root(action)\n",
    "\n",
    "    env.close()\n",
    "    return total_reward, step_count, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, steps, done = run_mcts_frozenlake(is_slippery=False, c_value=2.0, n_iter_mcts=2000, rollout_policy=\"random\")\n",
    "print(f\"Resultado FrozenLake: Done = {done}, Reward = {r}, Steps = {steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, steps, done = run_mcts_frozenlake(is_slippery=True, c_value=1.0, n_iter_mcts=2000, rollout_policy=\"random\")\n",
    "print(f\"Resultado FrozenLake: Done = {done}, Reward = {r}, Steps = {steps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Crear un entorno para un Robot Pick&Place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos importando las librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sistemas de recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos 3 sistemas distintos de recompensa:\n",
    "- Recompensa simple por objetivos alcanzados.\n",
    "- Recompensa gradual por acercarse al objetivo.\n",
    "- Recompensa mixta (combinación de hitos y distancias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward_simple(env):\n",
    "    \"\"\"\n",
    "    Calcula la recompensa basada en objetivos alcanzados y penaliza soltar la pieza fuera del objetivo.\n",
    "\n",
    "    Args:\n",
    "        env (Environment): El entorno del robot (posiciones y estados).\n",
    "\n",
    "    Returns:\n",
    "        float: Recompensa acumulada según el estado actual.\n",
    "    \"\"\"\n",
    "    reward = -0.01  # Penalización por cada paso realizado.\n",
    "\n",
    "    # Recompensa por recoger la pieza.\n",
    "    if env.has_piece == 1 and env.piece_x_position == -1 and not hasattr(env, 'piece_collected'):\n",
    "        reward += 50\n",
    "        setattr(env, 'piece_collected', True)\n",
    "\n",
    "    # Recompensa por llevar la pieza al objetivo.\n",
    "    if (env.piece_x_position == env.goal_x_position and\n",
    "        env.piece_y_position == env.goal_y_position and\n",
    "        env.piece_z_position == env.goal_z_position and\n",
    "        not hasattr(env, 'goal_reached')):\n",
    "        reward += 70\n",
    "        setattr(env, 'goal_reached', True)\n",
    "\n",
    "    # Penalización por soltar la pieza en una posición incorrecta.\n",
    "    if env.has_piece == 0 and not hasattr(env, 'piece_placed') and (\n",
    "        env.piece_x_position != env.goal_x_position or\n",
    "        env.piece_y_position != env.goal_y_position or\n",
    "        env.piece_z_position != env.goal_z_position\n",
    "    ):\n",
    "        reward -= 30  # Penalización significativa.\n",
    "        setattr(env, 'piece_placed', True)\n",
    "\n",
    "    # Penalización por movimientos redundantes.\n",
    "    if hasattr(env, 'visited_positions'):\n",
    "        current_position = (env.robot_x_position, env.robot_y_position, env.robot_z_position)\n",
    "        if current_position in env.visited_positions:\n",
    "            reward -= 0.01\n",
    "        else:\n",
    "            env.visited_positions.add(current_position)\n",
    "    else:\n",
    "        env.visited_positions = set([(env.robot_x_position, env.robot_y_position, env.robot_z_position)])\n",
    "\n",
    "    return reward\n",
    "\n",
    "def calculate_reward_distance(env):\n",
    "    \"\"\"\n",
    "    Calcula la recompensa basada en la distancia al objetivo y los objetivos alcanzados,\n",
    "    penalizando además soltar la pieza en una posición incorrecta.\n",
    "\n",
    "    Args:\n",
    "        env (Environment): El entorno que contiene el estado actual del robot. \n",
    "\n",
    "    Returns:\n",
    "        float: La recompensa calculada para el estado actual del entorno.\n",
    "    \"\"\"\n",
    "    reward = 0  # Penalización inicial.\n",
    "\n",
    "    if env.has_piece == 0:\n",
    "        # Penalización proporcional a la distancia al objeto.\n",
    "        distance_to_piece = ((env.robot_x_position - env.piece_x_position) ** 2 +\n",
    "                             (env.robot_y_position - env.piece_y_position) ** 2 +\n",
    "                             (env.robot_z_position - env.piece_z_position) ** 2) ** 0.5\n",
    "        reward += distance_to_piece * 2\n",
    "    else:\n",
    "        # Penalización proporcional a la distancia al objetivo.\n",
    "        distance_to_goal = ((env.robot_x_position - env.goal_x_position) ** 2 +\n",
    "                            (env.robot_y_position - env.goal_y_position) ** 2 +\n",
    "                            (env.robot_z_position - env.goal_z_position) ** 2) ** 0.5\n",
    "        reward += distance_to_goal * 2\n",
    "\n",
    "    # Recompensa por recoger la pieza.\n",
    "    if env.has_piece == 1 and not hasattr(env, 'piece_collected'):\n",
    "        reward += 50\n",
    "        setattr(env, 'piece_collected', True)\n",
    "\n",
    "    # Recompensa por alcanzar el objetivo.\n",
    "    if (env.piece_x_position == env.goal_x_position and\n",
    "        env.piece_y_position == env.goal_y_position and\n",
    "        env.piece_z_position == env.goal_z_position and\n",
    "        not hasattr(env, 'goal_reached')):\n",
    "        reward += 70\n",
    "        setattr(env, 'goal_reached', True)\n",
    "\n",
    "    # Penalización por soltar la pieza en una posición incorrecta.\n",
    "    if env.has_piece == 0 and not hasattr(env, 'piece_placed') and (\n",
    "        env.piece_x_position != env.goal_x_position or\n",
    "        env.piece_y_position != env.goal_y_position or\n",
    "        env.piece_z_position != env.goal_z_position\n",
    "    ):\n",
    "        reward -= 30\n",
    "        setattr(env, 'piece_placed', True)\n",
    "\n",
    "    # Penalización por movimientos redundantes.\n",
    "    if hasattr(env, 'visited_positions'):\n",
    "        current_position = (env.robot_x_position, env.robot_y_position, env.robot_z_position)\n",
    "        if current_position in env.visited_positions:\n",
    "            reward -= 7\n",
    "        else:\n",
    "            env.visited_positions.add(current_position)\n",
    "    else:\n",
    "        env.visited_positions = set([(env.robot_x_position, env.robot_y_position, env.robot_z_position)])\n",
    "\n",
    "    return reward\n",
    "\n",
    "def calculate_reward_mixed(env):\n",
    "    \"\"\"\n",
    "    Calcula la recompensa mixta basada en objetivos alcanzados y distancia al objetivo,\n",
    "    penalizando además soltar la pieza en una posición incorrecta.\n",
    "\n",
    "    Args:\n",
    "        env (Environment): El entorno que contiene el estado actual del robot.\n",
    "\n",
    "    Returns:\n",
    "        float: La recompensa calculada para el estado actual del entorno.\n",
    "    \"\"\"\n",
    "    reward = -0.25  # Penalización por cada paso realizado.\n",
    "\n",
    "    # Penalización basada en la distancia.\n",
    "    if env.has_piece == 0:\n",
    "        distance_to_piece = ((env.robot_x_position - env.piece_x_position) ** 2 +\n",
    "                             (env.robot_y_position - env.piece_y_position) ** 2 +\n",
    "                             (env.robot_z_position - env.piece_z_position) ** 2) ** 0.5\n",
    "        reward += distance_to_piece * 1.75\n",
    "    else:\n",
    "        distance_to_goal = ((env.robot_x_position - env.goal_x_position) ** 2 +\n",
    "                            (env.robot_y_position - env.goal_y_position) ** 2 +\n",
    "                            (env.robot_z_position - env.goal_z_position) ** 2) ** 0.5\n",
    "        reward += distance_to_goal * 1.75\n",
    "\n",
    "    # Recompensa por recoger la pieza.\n",
    "    if env.has_piece == 1 and not hasattr(env, 'piece_collected'):\n",
    "        reward += 50\n",
    "        setattr(env, 'piece_collected', True)\n",
    "\n",
    "    # Recompensa por alcanzar el objetivo.\n",
    "    if (env.piece_x_position == env.goal_x_position and\n",
    "        env.piece_y_position == env.goal_y_position and\n",
    "        env.piece_z_position == env.goal_z_position and\n",
    "        not hasattr(env, 'goal_reached')):\n",
    "        reward += 70\n",
    "        setattr(env, 'goal_reached', True)\n",
    "\n",
    "    # Penalización por soltar la pieza en una posición incorrecta.\n",
    "    if env.has_piece == 0 and not hasattr(env, 'piece_placed') and (\n",
    "        env.piece_x_position != env.goal_x_position or\n",
    "        env.piece_y_position != env.goal_y_position or\n",
    "        env.piece_z_position != env.goal_z_position\n",
    "    ):\n",
    "        reward -= 30  # Penalización significativa.\n",
    "        setattr(env, 'piece_placed', True)\n",
    "\n",
    "    # Penalización por movimientos redundantes.\n",
    "    if hasattr(env, 'visited_positions'):\n",
    "        current_position = (env.robot_x_position, env.robot_y_position, env.robot_z_position)\n",
    "        if current_position in env.visited_positions:\n",
    "            reward -= 6\n",
    "        else:\n",
    "            env.visited_positions.add(current_position)\n",
    "    else:\n",
    "        env.visited_positions = set([(env.robot_x_position, env.robot_y_position, env.robot_z_position)])\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear el entorno, creamos una clase. \n",
    "Dentro de ella, definimos el constructor, donde definimos el espacio de acciones y de observaciones. \n",
    "También, inicializamos el estado, creando las condiciones iniciales, y la visualización del robot.\n",
    "\n",
    "Podemos decir que generamos nuestros objetos, el robot y la pieza, al definir sus posiciones; y establecemos el objetivo: la posición a la que el robot debe llevar la pieza.\n",
    "\n",
    "Escogemos un sistema de recompensas. Al actuar de forma aleatoria, la configuración de este sistema no va a cambiar cómo actúa nuestro robot, pero sí es útil de cara a futuras implementaciones de algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotPickAndPlaceEnv(gymnasium.Env):\n",
    "    \"\"\"\n",
    "    Representa el entorno del robot para resolver el problema de pick & place.\n",
    "\n",
    "    Este entorno simula un espacio discreto en 3D donde un robot debe recoger una \n",
    "    pieza en un punto inicial y transportarla a un objetivo. El robot puede moverse \n",
    "    en las direcciones X, Y y Z, además de abrir o cerrar su pinza para coger y soltar la pieza.\n",
    "\n",
    "    Attributes:\n",
    "        robot_position: Posición actual del robot en (x, y, z).\n",
    "        piece_position: Posición actual de la pieza en (x, y, z). (-1, -1, -1 si está siendo transportada).\n",
    "        goal_position: Posición del objetivo en (x, y, z).\n",
    "        has_piece (bool): Indica si el robot tiene la pieza agarrada (1 si tiene la pieza, 0 en caso contrario).\n",
    "        step_limit (int): Número máximo de pasos permitidos.\n",
    "        action_space (gymnasium.spaces.Discrete): Espacio de acciones (0-6).\n",
    "        observation_space (gymnasium.spaces.Tuple): Espacio de observaciones del entorno.\n",
    "        reward_function: Función de recompensa utilizada para evaluar los pasos del robot.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno del robot.\n",
    "\n",
    "        Configura los espacios de acción y observación, y define las posiciones iniciales \n",
    "        del robot, la pieza y el objetivo.\n",
    "        \"\"\"\n",
    "        super(RobotPickAndPlaceEnv, self).__init__()\n",
    "\n",
    "        # Definir espacio de acciones: 0 = mover izquierda, 1 = mover derecha, 2 = mover abajo, 3 = mover arriba, 4 = mover atrás, 5 = mover delante, 6 = abrir/cerrar pinza\n",
    "        self.action_space = spaces.Discrete(7)\n",
    "\n",
    "        # Definir espacio de observaciones\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(10),  # Posición del robot en el eje X (0-9)\n",
    "            spaces.Discrete(10),  # Posición del robot en el eje Y (0-9)\n",
    "            spaces.Discrete(10),  # Posición del robot en el eje Z (0-9)\n",
    "            spaces.Discrete(2),   # Tiene pieza (0 o 1)\n",
    "            spaces.Discrete(11),  # Posición de la pieza en el eje X (0-9) (-1 si está siendo transportada)\n",
    "            spaces.Discrete(11),  # Posición de la pieza en el eje Y (0-9) (-1 si está siendo transportada)\n",
    "            spaces.Discrete(11)   # Posición de la pieza en el eje Z (0-9) (-1 si está siendo transportada)\n",
    "        ))\n",
    "\n",
    "        # Inicializar estado\n",
    "        self.reset()\n",
    "\n",
    "        # Inicializar figura, eje y gráfica de matplotlib\n",
    "        self.fig = plt.figure()\n",
    "        self.ax = self.fig.add_subplot(111, projection='3d')\n",
    "        self.ax.set_xlim(0, 10)\n",
    "        self.ax.set_ylim(0, 10)\n",
    "        self.ax.set_zlim(0, 10)\n",
    "        self.ax.set_xlabel('X')\n",
    "        self.ax.set_ylabel('Y')\n",
    "        self.ax.set_zlabel('Z')\n",
    "\n",
    "        self.robot_plot, = self.ax.plot([self.robot_x_position], [self.robot_y_position], [self.robot_z_position], 'ro', label=\"Robot\")\n",
    "        self.piece_plot, = self.ax.plot([self.piece_x_position], [self.piece_y_position], [self.piece_z_position], 'bo', label=\"Pieza\")\n",
    "        self.goal_plot, = self.ax.plot([self.goal_x_position], [self.goal_y_position], [self.goal_z_position], 'go', label=\"Objetivo\")\n",
    "        self.ax.legend()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reinicia el entorno a su estado inicial.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Observación inicial del entorno.\n",
    "        \"\"\"\n",
    "        # Condiciones iniciales\n",
    "        self.robot_x_position = 0 # El robot comienza en la posición (0,0,0)\n",
    "        self.robot_y_position = 0\n",
    "        self.robot_z_position = 0\n",
    "        self.has_piece = 0  # El robot no tiene la pieza\n",
    "        self.piece_x_position = 5  # La pieza comienza en la posición (5,5,5)\n",
    "        self.piece_y_position = 5\n",
    "        self.piece_z_position = 5\n",
    "        self.goal_x_position = 9  # El objetivo está en la posición (9, 7, 9)\n",
    "        self.goal_y_position = 7\n",
    "        self.goal_z_position = 9\n",
    "\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Realiza un paso en el entorno según la acción proporcionada.\n",
    "\n",
    "        Args:\n",
    "            action (int): Índice de la acción a realizar (0-6).\n",
    "\n",
    "        Returns:\n",
    "            tuple: Estado actual del entorno, recompensa obtenida, \n",
    "                indicador de si la tarea ha terminado, indicador de truncado, \n",
    "                y diccionario de información adicional.\n",
    "        \"\"\"\n",
    "        # Asegurarse de que la acción es válida\n",
    "        assert self.action_space.contains(action)\n",
    "\n",
    "        done = False\n",
    "        truncated = False\n",
    "\n",
    "        # Realizar movimientos según la acción\n",
    "        if action == 0:  # Mover izquierda\n",
    "            self.robot_x_position = max(0, self.robot_x_position - 1)\n",
    "        elif action == 1:  # Mover derecha\n",
    "            self.robot_x_position = min(9, self.robot_x_position + 1)\n",
    "        elif action == 2:  # Mover abajo\n",
    "            self.robot_y_position = max(0, self.robot_y_position - 1)\n",
    "        elif action == 3:  # Mover arriba\n",
    "            self.robot_y_position = min(9, self.robot_y_position + 1)\n",
    "        elif action == 4:  # Mover atrás\n",
    "            self.robot_z_position = max(0, self.robot_z_position - 1)\n",
    "        elif action == 5:  # Mover adelante\n",
    "            self.robot_z_position = min(9, self.robot_z_position + 1)\n",
    "        elif action == 6:  # Abrir/cerrar pinza\n",
    "            if not self.has_piece:\n",
    "                if (self.robot_x_position == self.piece_x_position and\n",
    "                    self.robot_y_position == self.piece_y_position and\n",
    "                    self.robot_z_position == self.piece_z_position):\n",
    "                    print(\"¡La pieza ha sido recogida!\")\n",
    "                    self.has_piece = 1\n",
    "                    self.piece_x_position = -1\n",
    "                    self.piece_y_position = -1\n",
    "                    self.piece_z_position = -1\n",
    "                else:\n",
    "                    print(\"No hay pieza en esta posición.\")\n",
    "            else:\n",
    "                self.has_piece = 0\n",
    "                self.piece_x_position = self.robot_x_position\n",
    "                self.piece_y_position = self.robot_y_position\n",
    "                self.piece_z_position = self.robot_z_position\n",
    "                if (self.piece_x_position == self.goal_x_position and\n",
    "                    self.piece_y_position == self.goal_y_position and\n",
    "                    self.piece_z_position == self.goal_z_position):\n",
    "                    print(\"¡La pieza está en el objetivo!\")\n",
    "                    done = True\n",
    "\n",
    "        # Calcular la recompensa usando la función de recompensa seleccionada\n",
    "        if hasattr(self, 'reward_function') and callable(self.reward_function):\n",
    "            reward = self.reward_function(self)\n",
    "        else:\n",
    "            raise ValueError(\"La función de recompensa no está definida o no es válida.\")\n",
    "\n",
    "        # Mostrar el estado del robot y pieza\n",
    "        print(f\"Robot: ({self.robot_x_position}, {self.robot_y_position}, {self.robot_z_position}) | \"\n",
    "            f\"Pieza: ({self.piece_x_position}, {self.piece_y_position}, {self.piece_z_position}) | \"\n",
    "            f\"Has_piece: {self.has_piece}\")\n",
    "\n",
    "        return self._get_observation(), reward, done, truncated, {}\n",
    "\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Renderiza el estado actual del entorno en una gráfica 3D.\n",
    "        \"\"\"\n",
    "        # Actualizar posición del robot\n",
    "        self.robot_plot.set_data([self.robot_x_position], [self.robot_y_position])\n",
    "        self.robot_plot.set_3d_properties([self.robot_z_position])\n",
    "\n",
    "        # Actualizar posición de la pieza\n",
    "        if self.has_piece:\n",
    "            self.piece_plot.set_data([self.robot_x_position], [self.robot_y_position])\n",
    "            self.piece_plot.set_3d_properties([self.robot_z_position])\n",
    "        else:\n",
    "            self.piece_plot.set_data([self.piece_x_position], [self.piece_y_position])\n",
    "            self.piece_plot.set_3d_properties([self.piece_z_position])\n",
    "\n",
    "        plt.pause(0.25)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Obtiene la observación actual del entorno.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Estado actual del entorno.\n",
    "        \"\"\"\n",
    "        return (self.robot_x_position, self.robot_y_position, self.robot_z_position, self.has_piece, self.piece_x_position, self.piece_y_position, self.piece_z_position)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Cierra la visualización del entorno.\n",
    "        \"\"\"\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tenemos 7 espacios de acciones posibles.\n",
    "\n",
    "- (1000 (posiciones del robot) × 2 (pinza) × 1001 (posiciones de la pieza)) = 2.002.000 estados posibles.\n",
    "\n",
    "- 2.002.000×7 = 14.014.000 combinaciones estado-acción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos y reiniciamos el entorno, y seguimos una trayectoria óptima y otra subóptima para ver como funciona cada sistema de recompensas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando función de recompensa: Simple\n",
      "\n",
      "Trayectoria óptima:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 14:45:24.334 python[11982:1135714] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-04 14:45:24.334 python[11982:1135714] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robot: (1, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (2, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (3, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 1, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 2, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 3, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 4, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 1) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 2) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 3) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 4) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 5) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "¡La pieza ha sido recogida!\n",
      "Robot: (5, 5, 5) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 7) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 8) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (6, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (7, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (8, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 6, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 7, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "¡La pieza está en el objetivo!\n",
      "Robot: (9, 7, 9) | Pieza: (9, 7, 9) | Has_piece: 0\n",
      "Recompensa total acumulada: 89.70999999999995\n",
      "\n",
      "Trayectoria subóptima:\n",
      "Robot: (1, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (2, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (3, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 1, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 2, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 3, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 4, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 1) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 2) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 3) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 2) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 2) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 3) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 4) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 5) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "¡La pieza ha sido recogida!\n",
      "Robot: (5, 5, 5) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 7) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 5) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 6, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (6, 6, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (7, 6, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (7, 7, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (7, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (8, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 6) | Pieza: (9, 8, 6) | Has_piece: 0\n",
      "Robot: (9, 7, 6) | Pieza: (9, 8, 6) | Has_piece: 0\n",
      "Robot: (9, 8, 6) | Pieza: (9, 8, 6) | Has_piece: 0\n",
      "¡La pieza ha sido recogida!\n",
      "Robot: (9, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 5) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 7) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 8) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 7, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "¡La pieza está en el objetivo!\n",
      "Robot: (9, 7, 9) | Pieza: (9, 7, 9) | Has_piece: 0\n",
      "Recompensa total acumulada: -0.6600000000000003\n",
      "\n",
      "Evaluando función de recompensa: Distance\n",
      "\n",
      "Trayectoria óptima:\n",
      "Robot: (1, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (2, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (3, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 1, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 2, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 3, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 4, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 1) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 2) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 3) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 4) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 5) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "¡La pieza ha sido recogida!\n",
      "Robot: (5, 5, 5) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 7) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 8) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (6, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (7, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (8, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 6, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 7, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "¡La pieza está en el objetivo!\n",
      "Robot: (9, 7, 9) | Pieza: (9, 7, 9) | Has_piece: 0\n",
      "Recompensa total acumulada: 35.18662478926822\n",
      "\n",
      "Trayectoria subóptima:\n",
      "Robot: (1, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (2, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (3, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 1, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 2, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 3, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 4, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 1) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 2) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 3) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 2) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 2) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 3) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 4) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 5) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "¡La pieza ha sido recogida!\n",
      "Robot: (5, 5, 5) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 7) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 5) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 6, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (6, 6, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (7, 6, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (7, 7, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (7, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (8, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 6) | Pieza: (9, 8, 6) | Has_piece: 0\n",
      "Robot: (9, 7, 6) | Pieza: (9, 8, 6) | Has_piece: 0\n",
      "Robot: (9, 8, 6) | Pieza: (9, 8, 6) | Has_piece: 0\n",
      "¡La pieza ha sido recogida!\n",
      "Robot: (9, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 5) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 7) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 8) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 7, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "¡La pieza está en el objetivo!\n",
      "Robot: (9, 7, 9) | Pieza: (9, 7, 9) | Has_piece: 0\n",
      "Recompensa total acumulada: 13.976819470688767\n",
      "\n",
      "Evaluando función de recompensa: Mixed\n",
      "\n",
      "Trayectoria óptima:\n",
      "Robot: (1, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (2, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (3, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 1, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 2, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 3, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 4, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 1) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 2) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 3) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 4) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 5) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "¡La pieza ha sido recogida!\n",
      "Robot: (5, 5, 5) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 7) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 8) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (6, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (7, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (8, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 5, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 6, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 7, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "¡La pieza está en el objetivo!\n",
      "Robot: (9, 7, 9) | Pieza: (9, 7, 9) | Has_piece: 0\n",
      "Recompensa total acumulada: 27.413296690609705\n",
      "\n",
      "Trayectoria subóptima:\n",
      "Robot: (1, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (2, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (3, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 0, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 1, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 2, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 3, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 4, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 0) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 1) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 2) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 3) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (4, 5, 2) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 2) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 3) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 4) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "Robot: (5, 5, 5) | Pieza: (5, 5, 5) | Has_piece: 0\n",
      "¡La pieza ha sido recogida!\n",
      "Robot: (5, 5, 5) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 7) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 5) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 5, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (5, 6, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (6, 6, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (7, 6, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (7, 7, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (7, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (8, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 6) | Pieza: (9, 8, 6) | Has_piece: 0\n",
      "Robot: (9, 7, 6) | Pieza: (9, 8, 6) | Has_piece: 0\n",
      "Robot: (9, 8, 6) | Pieza: (9, 8, 6) | Has_piece: 0\n",
      "¡La pieza ha sido recogida!\n",
      "Robot: (9, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 5) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 6) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 7) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 8) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 8, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "Robot: (9, 7, 9) | Pieza: (-1, -1, -1) | Has_piece: 1\n",
      "¡La pieza está en el objetivo!\n",
      "Robot: (9, 7, 9) | Pieza: (9, 7, 9) | Has_piece: 0\n",
      "Recompensa total acumulada: 6.854717036852655\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Inicializamos el entorno\n",
    "    env = RobotPickAndPlaceEnv()\n",
    "\n",
    "    optimal_trajectory = [\n",
    "        (1, 0),  # Mover +X\n",
    "        (1, 0),  # Mover +X\n",
    "        (1, 0),  # Mover +X\n",
    "        (1, 0),  # Mover +X\n",
    "        (1, 0),  # Mover +X\n",
    "        (3, 0),  # Mover +Y\n",
    "        (3, 0),  # Mover +Y\n",
    "        (3, 0),  # Mover +Y\n",
    "        (3, 0),  # Mover +Y\n",
    "        (3, 0),  # Mover +Y\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z\n",
    "        (6, 0),  # Cerrar pinza\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z\n",
    "        (1, 0),  # Mover +X\n",
    "        (1, 0),  # Mover +X\n",
    "        (1, 0),  # Mover +X\n",
    "        (1, 0),  # Mover +X\n",
    "        (3, 0),  # Mover +Y\n",
    "        (3, 0),  # Mover +Y\n",
    "        (6, 0)   # Abrir pinza\n",
    "    ]\n",
    "\n",
    "    suboptimal_trajectory = [\n",
    "        (1, 0),  # Mover +X\n",
    "        (1, 0),  # Mover +X\n",
    "        (1, 0),  # Mover +X\n",
    "        (1, 0),  # Mover +X\n",
    "        (3, 0),  # Mover +Y\n",
    "        (3, 0),  # Mover +Y\n",
    "        (3, 0),  # Mover +Y\n",
    "        (3, 0),  # Mover +Y\n",
    "        (3, 0),  # Mover +Y\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z\n",
    "        (4, 0),  # Mover -Y (innecesario)\n",
    "        (1, 0),  # Mover +X\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z (innecesario)\n",
    "        (6, 0),  # Abrir pinza\n",
    "        (5, 0),  # Mover +Z (innecesario)\n",
    "        (5, 0),  # Mover +Z (innecesario)\n",
    "        (4, 0),  # Mover -Y (innecesario)\n",
    "        (4, 0),  # Mover -Y (innecesario)\n",
    "        (5, 0),  # Mover +Z (innecesario)\n",
    "        (3, 0),  # Mover +Y (innecesario)\n",
    "        (1, 0),  # Mover +X\n",
    "        (1, 0),  # Mover +X\n",
    "        (3, 0),  # Mover +Y\n",
    "        (3, 0),  # Mover +Y\n",
    "        (1, 0),  # Mover +X\n",
    "        (1, 0),  # Mover +X\n",
    "        (6, 0),  # Abrir pinza (innecesario)\n",
    "        (2, 0),  # Mover -Y (innecesario)\n",
    "        (3, 0),  # Mover +Y (innecesario)\n",
    "        (6, 0),  # Cerrar pinza (innecesario)\n",
    "        (4, 0),  # Mover -Z (innecesario)\n",
    "        (5, 0),  # Mover +Z (innecesario)\n",
    "        (1, 0),  # Mover +X\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z\n",
    "        (5, 0),  # Mover +Z\n",
    "        (2, 0),  # Mover -Y\n",
    "        (6, 0)   # Abrir pinza\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Funciones de recompensa a probar\n",
    "    reward_functions = [\n",
    "        (\"Simple\", calculate_reward_simple),\n",
    "        (\"Distance\", calculate_reward_distance),\n",
    "        (\"Mixed\", calculate_reward_mixed)\n",
    "    ]\n",
    "\n",
    "    for name, reward_function in reward_functions:\n",
    "        env.reward_function = reward_function\n",
    "\n",
    "        print(f\"\\nEvaluando función de recompensa: {name}\")\n",
    "\n",
    "        # Evaluar trayectoria óptima\n",
    "        print(\"\\nTrayectoria óptima:\")\n",
    "        env.reset()\n",
    "        optimal_reward = 0\n",
    "        for action, _ in optimal_trajectory:\n",
    "            env.render()  # Mostrar visualización en cada paso\n",
    "            _, reward, done, _, _ = env.step(action)\n",
    "            optimal_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"Recompensa total acumulada: {optimal_reward}\")\n",
    "\n",
    "        # Evaluar trayectoria subóptima\n",
    "        print(\"\\nTrayectoria subóptima:\")\n",
    "        env.reset()\n",
    "        suboptimal_reward = 0\n",
    "        for action, _ in suboptimal_trajectory:\n",
    "            env.render()  # Mostrar visualización en cada paso\n",
    "            _, reward, done, _, _ = env.step(action)\n",
    "            suboptimal_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"Recompensa total acumulada: {suboptimal_reward}\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Función de Recompensa Simple: Esta función valora de manera significativa el progreso hacia los objetivos y permite un margen de exploración porque las penalizaciones son menos severas. Podría penalizar más las trayectorias menos óptimas.\n",
    "\n",
    "- Función de Recompensa Distancia: Esta función penaliza la distancia aunque no es muy agresiva. Se podría aumentar la penalización por movimientos redundantes.\n",
    "\n",
    "- Función de Recompensa Mixta: Esta función ofrece un buen balance entre objetivos y distancia. Al igual que en la Simple, podría penalizar más las trayectorias menos óptimas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

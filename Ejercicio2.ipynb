{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Implementar Monte-Carlo ES y Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sistemas de recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos 3 sistemas distintos de recompensa:\n",
    "- Recompensa simple por objetivos alcanzados.\n",
    "- Recompensa de disatncia.\n",
    "- Recompensa mixta (combinación de objetivos y distancias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward_simple(env):\n",
    "    \"\"\"\n",
    "    Calcula la recompensa basada únicamente en objetivos alcanzados y penalizaciones claras.\n",
    "\n",
    "    Args:\n",
    "        env (Environment): El entorno del robot (posiciones y estados).\n",
    "\n",
    "    Returns:\n",
    "        float: Recompensa acumulada según el estado actual.\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "\n",
    "    # Penalización por intentar recoger la pieza en una posición incorrecta.\n",
    "    if env.has_piece == 0 and env.action_last == 6 and (\n",
    "        env.robot_x_position != env.piece_x_position or\n",
    "        env.robot_y_position != env.piece_y_position or\n",
    "        env.robot_z_position != env.piece_z_position\n",
    "    ):\n",
    "        reward -= 50\n",
    "\n",
    "    # Recompensa por estar donde la pieza.\n",
    "    if env.robot_x_position == env.piece_x_position and env.robot_y_position == env.piece_y_position and env.robot_z_position == env.piece_z_position and env.has_piece == 0:\n",
    "        reward += 10\n",
    "\n",
    "    # Recompensa por recoger la pieza (solo una vez).\n",
    "    if env.has_piece == 1 and not hasattr(env, 'piece_collected'):\n",
    "        reward += 100\n",
    "        setattr(env, 'piece_collected', True)\n",
    "\n",
    "    # Penalización por abrir/cerrar pinza de forma repetitiva.\n",
    "    if not hasattr(env, 'pinza_toggle_count'):\n",
    "        env.pinza_toggle_count = 0\n",
    "\n",
    "    if env.action_last == 6:\n",
    "        env.pinza_toggle_count += 1\n",
    "        if env.pinza_toggle_count > 2: # Penalizar solo si ocurre más de dos veces\n",
    "            reward -= 500\n",
    "    else:\n",
    "        env.pinza_toggle_count = 0  # Reinicia si realiza otra acción\n",
    "\n",
    "    # Recompensa por progresar hacia el objetivo en los ejes.\n",
    "    if env.has_piece == 1:\n",
    "        aligned_axes = sum([\n",
    "            env.robot_x_position == env.goal_x_position,\n",
    "            env.robot_y_position == env.goal_y_position,\n",
    "            env.robot_z_position == env.goal_z_position\n",
    "        ])\n",
    "        reward += 50 * aligned_axes\n",
    "\n",
    "    # Recompensa por estar alineado donde el objetivo mientras lleva la pieza.\n",
    "    if env.has_piece == 1 and env.robot_x_position == env.goal_x_position and env.robot_y_position == env.goal_y_position and env.robot_z_position == env.goal_z_position:\n",
    "        reward += 500\n",
    "\n",
    "    # Recompensa por llevar la pieza al objetivo.\n",
    "    if (env.piece_x_position == env.goal_x_position and\n",
    "        env.piece_y_position == env.goal_y_position and\n",
    "        env.piece_z_position == env.goal_z_position and\n",
    "        not hasattr(env, 'goal_reached')):\n",
    "        reward += 500\n",
    "        setattr(env, 'goal_reached', True)\n",
    "\n",
    "    # Recompensa por soltar la pieza en el objetivo.\n",
    "    if env.has_piece == 0 and env.robot_x_position == env.goal_x_position and env.robot_y_position == env.goal_y_position and env.robot_z_position == env.goal_z_position:\n",
    "        reward += 5000\n",
    "\n",
    "    # Penalización por soltar la pieza fuera del objetivo.\n",
    "    if env.has_piece == 0 and not hasattr(env, 'piece_placed') and (\n",
    "        env.piece_x_position != env.goal_x_position or\n",
    "        env.piece_y_position != env.goal_y_position or\n",
    "        env.piece_z_position != env.goal_z_position\n",
    "    ):\n",
    "        reward -= 1000\n",
    "        setattr(env, 'piece_placed', True)\n",
    "\n",
    "    # Penalización adicional por soltar repetidamente la pieza en posiciones incorrectas.\n",
    "    if not hasattr(env, 'incorrect_drop_count'):\n",
    "        env.incorrect_drop_count = 0\n",
    "\n",
    "    if env.has_piece == 0 and (\n",
    "        env.piece_x_position != env.goal_x_position or\n",
    "        env.piece_y_position != env.goal_y_position or\n",
    "        env.piece_z_position != env.goal_z_position\n",
    "    ):\n",
    "        env.incorrect_drop_count += 1\n",
    "        if env.incorrect_drop_count > 1:  # Penalizar solo si ocurre más de una vez\n",
    "            reward -= 500 * env.incorrect_drop_count\n",
    "    else:\n",
    "        env.incorrect_drop_count = 0  # Reiniciar el contador si la pieza está en el lugar correcto\n",
    "\n",
    "    # Recompensa constante por mantener la pieza.\n",
    "    if env.has_piece == 1 and not (env.robot_x_position == env.goal_x_position and env.robot_y_position == env.goal_y_position and env.robot_z_position == env.goal_z_position):\n",
    "        reward += 50\n",
    "\n",
    "    # Penalización por movimientos redundantes.\n",
    "    if hasattr(env, 'visited_positions'):\n",
    "        current_position = (env.robot_x_position, env.robot_y_position, env.robot_z_position)\n",
    "        if current_position in env.visited_positions:\n",
    "            reward -= 50\n",
    "        else:\n",
    "            env.visited_positions.add(current_position)\n",
    "    else:\n",
    "        env.visited_positions = set([(env.robot_x_position, env.robot_y_position, env.robot_z_position)])\n",
    "\n",
    "    # Penalización por entrar en bucles.\n",
    "    if hasattr(env, 'recent_positions'):\n",
    "        current_position = (env.robot_x_position, env.robot_y_position, env.robot_z_position)\n",
    "        env.recent_positions.append(current_position)\n",
    "        # Limitar el historial a las últimas 10 posiciones.\n",
    "        if len(env.recent_positions) > 10:\n",
    "            env.recent_positions.pop(0)\n",
    "        # Penalizar bucles.\n",
    "        if len(env.recent_positions) > len(set(env.recent_positions)):\n",
    "            reward -= 200\n",
    "    else:\n",
    "        env.recent_positions = [(env.robot_x_position, env.robot_y_position, env.robot_z_position)]\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward_distance(env):\n",
    "    \"\"\"\n",
    "    Calcula la recompensa únicamente en función de las distancias al objetivo y la pieza.\n",
    "\n",
    "    Args:\n",
    "        env (Environment): El entorno que contiene el estado actual del robot. \n",
    "\n",
    "    Returns:\n",
    "        float: La recompensa calculada para el estado actual del entorno.\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "\n",
    "    # Calcular la distancia al objeto\n",
    "    distance_to_piece = ((env.robot_x_position - env.piece_x_position) ** 2 +\n",
    "                         (env.robot_y_position - env.piece_y_position) ** 2 +\n",
    "                         (env.robot_z_position - env.piece_z_position) ** 2) ** 0.5\n",
    "\n",
    "    # Calcular la distancia al objetivo\n",
    "    distance_to_goal = ((env.robot_x_position - env.goal_x_position) ** 2 +\n",
    "                        (env.robot_y_position - env.goal_y_position) ** 2 +\n",
    "                        (env.robot_z_position - env.goal_z_position) ** 2) ** 0.5\n",
    "\n",
    "    if env.has_piece == 0:\n",
    "        # Recompensa basada en la cercanía al objeto\n",
    "        reward += max(0, 100 - distance_to_piece * 10)\n",
    "    else:\n",
    "        # Recompensa basada en la cercanía al objetivo\n",
    "        reward += max(0, 500 - distance_to_goal * 10)\n",
    "\n",
    "        # Recompensa adicional por reducir la distancia al objetivo\n",
    "        if hasattr(env, 'previous_distance_to_goal'):\n",
    "            if distance_to_goal < env.previous_distance_to_goal:\n",
    "                reward += 50\n",
    "        env.previous_distance_to_goal = distance_to_goal\n",
    "\n",
    "    piece_distance_to_goal = ((env.piece_x_position - env.goal_x_position) ** 2 +\n",
    "                            (env.piece_y_position - env.goal_y_position) ** 2 +\n",
    "                            (env.piece_z_position - env.goal_z_position) ** 2) ** 0.5\n",
    "\n",
    "    # Recompensa por llevar la pieza al objetivo\n",
    "    if piece_distance_to_goal == 0 and env.has_piece == 0:\n",
    "        reward += 100000\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_reward_mixed(env):\n",
    "    \"\"\"\n",
    "    Calcula la recompensa mixta basada en objetivos alcanzados y distancia al objetivo. Añade penalizaciones por movimientos redundantes y bucles.\n",
    "\n",
    "    Args:\n",
    "        env (Environment): El entorno que contiene el estado actual del robot.\n",
    "\n",
    "    Returns:\n",
    "        float: La recompensa calculada para el estado actual del entorno.\n",
    "    \"\"\"\n",
    "    reward = 0 \n",
    "\n",
    "    if env.has_piece == 0:\n",
    "        # Recompensa proporcional a la cercanía al objeto.\n",
    "        distance_to_piece = ((env.robot_x_position - env.piece_x_position) ** 2 +\n",
    "                             (env.robot_y_position - env.piece_y_position) ** 2 +\n",
    "                             (env.robot_z_position - env.piece_z_position) ** 2) ** 0.5\n",
    "        reward += max(0, 50 - distance_to_piece * 2)\n",
    "    else:\n",
    "        # Recompensa proporcional a la cercanía al objetivo.\n",
    "        distance_to_goal = ((env.robot_x_position - env.goal_x_position) ** 2 +\n",
    "                            (env.robot_y_position - env.goal_y_position) ** 2 +\n",
    "                            (env.robot_z_position - env.goal_z_position) ** 2) ** 0.5\n",
    "        reward += max(0, 200 - distance_to_goal * 2)\n",
    "\n",
    "        # Penalización y recompensa acumulativa basada en la distancia al objetivo.\n",
    "        if hasattr(env, 'previous_distance_to_goal'):\n",
    "            if distance_to_goal > env.previous_distance_to_goal:\n",
    "                reward -= 5000\n",
    "            else:\n",
    "                reward += 10000\n",
    "        env.previous_distance_to_goal = distance_to_goal\n",
    "\n",
    "    # Recompensa por llegar a la pieza.\n",
    "    if env.piece_x_position == env.robot_x_position and env.piece_y_position == env.robot_y_position and env.piece_z_position == env.robot_z_position:\n",
    "        reward += 5000\n",
    "\n",
    "    # Recompensa por recoger la pieza (solo una vez).\n",
    "    if env.has_piece == 1 and not hasattr(env, 'piece_collected'):\n",
    "        reward += 100\n",
    "        setattr(env, 'piece_collected', True)\n",
    "\n",
    "    # Recompensa constante por mantener la pieza.\n",
    "    if env.has_piece == 1:\n",
    "        reward += 10000\n",
    "\n",
    "    # Recompensa por llegar al objetivo con la pieza.\n",
    "    if env.piece_x_position == env.robot_x_position == env.goal_x_position and env.piece_y_position == env.robot_y_position == env.goal_y_position and env.piece_z_position == env.robot_z_position == env.goal_z_position:\n",
    "        reward += 500000\n",
    "\n",
    "    # Recompensa por alcanzar el objetivo.\n",
    "    if (env.piece_x_position == env.goal_x_position and\n",
    "        env.piece_y_position == env.goal_y_position and\n",
    "        env.piece_z_position == env.goal_z_position and\n",
    "        not hasattr(env, 'goal_reached')):\n",
    "        reward += 1000000\n",
    "        setattr(env, 'goal_reached', True)\n",
    "\n",
    "    # Penalización por soltar la pieza en una posición incorrecta.\n",
    "    if env.has_piece == 0 and not hasattr(env, 'piece_placed') and (\n",
    "        env.piece_x_position != env.goal_x_position or\n",
    "        env.piece_y_position != env.goal_y_position or\n",
    "        env.piece_z_position != env.goal_z_position\n",
    "    ):\n",
    "        reward -= 10000\n",
    "        setattr(env, 'piece_placed', True)\n",
    "\n",
    "    # Penalización por movimientos redundantes.\n",
    "    if hasattr(env, 'visited_positions'):\n",
    "        current_position = (env.robot_x_position, env.robot_y_position, env.robot_z_position)\n",
    "        if current_position in env.visited_positions:\n",
    "            reward -= 50\n",
    "        else:\n",
    "            env.visited_positions.add(current_position)\n",
    "    else:\n",
    "        env.visited_positions = set([(env.robot_x_position, env.robot_y_position, env.robot_z_position)])\n",
    "\n",
    "    # Penalización por entrar en bucles.\n",
    "    if hasattr(env, 'recent_positions'):\n",
    "        current_position = (env.robot_x_position, env.robot_y_position, env.robot_z_position)\n",
    "        env.recent_positions.append(current_position)\n",
    "        # Limitar el historial a las últimas 10 posiciones.\n",
    "        if len(env.recent_positions) > 10:\n",
    "            env.recent_positions.pop(0)\n",
    "        # Penaliar bucles.\n",
    "        if len(env.recent_positions) > len(set(env.recent_positions)):\n",
    "            reward -= 20000\n",
    "    else:\n",
    "        env.recent_positions = [(env.robot_x_position, env.robot_y_position, env.robot_z_position)]\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotPickAndPlaceEnv(gymnasium.Env):\n",
    "    \"\"\"\n",
    "    Representa el entorno del robot para resolver el problema de pick & place.\n",
    "\n",
    "    Este entorno simula un espacio discreto en 3D donde un robot debe recoger una \n",
    "    pieza en un punto inicial y transportarla a un objetivo. El robot puede moverse \n",
    "    en las direcciones X, Y y Z, además de abrir o cerrar su pinza para coger y soltar la pieza.\n",
    "\n",
    "    Attributes:\n",
    "        robot_position: Posición actual del robot en (x, y, z).\n",
    "        piece_position: Posición actual de la pieza en (x, y, z). (-1, -1, -1 si está siendo transportada).\n",
    "        goal_position: Posición del objetivo en (x, y, z).\n",
    "        has_piece (bool): Indica si el robot tiene la pieza agarrada (1 si tiene la pieza, 0 en caso contrario).\n",
    "        step_limit (int): Número máximo de pasos permitidos.\n",
    "        action_space (gymnasium.spaces.Discrete): Espacio de acciones (0-6).\n",
    "        observation_space (gymnasium.spaces.Tuple): Espacio de observaciones del entorno.\n",
    "        reward_function: Función de recompensa utilizada para evaluar los pasos del robot.\n",
    "    \"\"\"\n",
    "    def __init__(self, reward_function=calculate_reward_simple):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno del robot y establece la función de recompensa, por defecto simple.\n",
    "\n",
    "        Configura los espacios de acción y observación, y define las posiciones iniciales \n",
    "        del robot, la pieza y el objetivo.\n",
    "        \"\"\"\n",
    "        super(RobotPickAndPlaceEnv, self).__init__()\n",
    "\n",
    "        # Definir espacio de acciones: 0 = mover izquierda, 1 = mover derecha, 2 = mover abajo, 3 = mover arriba, 4 = mover atrás, 5 = mover delante, 6 = abrir/cerrar pinza\n",
    "        self.action_space = spaces.Discrete(7)\n",
    "\n",
    "        # Definir espacio de observaciones\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(10),  # Posición del robot en el eje X (0-9)\n",
    "            spaces.Discrete(10),  # Posición del robot en el eje Y (0-9)\n",
    "            spaces.Discrete(10),  # Posición del robot en el eje Z (0-9)\n",
    "            spaces.Discrete(2),   # Tiene pieza (0 o 1)\n",
    "            spaces.Discrete(11),  # Posición de la pieza en el eje X (0-9) (-1 si está siendo transportada)\n",
    "            spaces.Discrete(11),  # Posición de la pieza en el eje Y (0-9) (-1 si está siendo transportada)\n",
    "            spaces.Discrete(11)   # Posición de la pieza en el eje Z (0-9) (-1 si está siendo transportada)\n",
    "        ))\n",
    "\n",
    "        # Establecer función de recompensa\n",
    "        self.reward_function = reward_function\n",
    "        \n",
    "        # Inicializar estado\n",
    "        self.reset()\n",
    "\n",
    "        # Inicializar figura, eje y gráfica de matplotlib\n",
    "        self.fig = None\n",
    "        self.ax = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reinicia el entorno a su estado inicial.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Observación inicial del entorno.\n",
    "        \"\"\"\n",
    "        # Condiciones iniciales\n",
    "        self.robot_x_position = 0  # El robot comienza en la posición (0,0,0)\n",
    "        self.robot_y_position = 0\n",
    "        self.robot_z_position = 0\n",
    "        self.has_piece = 0         # El robot no tiene la pieza\n",
    "        self.piece_x_position = 5  # La pieza comienza en la posición (5,5,5)\n",
    "        self.piece_y_position = 5\n",
    "        self.piece_z_position = 5\n",
    "        self.goal_x_position = 9   # El objetivo está en la posición (9, 7, 9)\n",
    "        self.goal_y_position = 7\n",
    "        self.goal_z_position = 9\n",
    "        self.steps = 0             # Contador de pasos por episodio\n",
    "\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Realiza un paso en el entorno según la acción proporcionada.\n",
    "\n",
    "        Args:\n",
    "            action (int): Índice de la acción a realizar (0-6).\n",
    "\n",
    "        Returns:\n",
    "            tuple: Estado actual del entorno, recompensa obtenida, \n",
    "                indicador de si la tarea ha terminado, indicador de truncado, \n",
    "                y diccionario de información adicional.\n",
    "        \"\"\"\n",
    "        # Asegurarse de que la acción es válida\n",
    "        assert self.action_space.contains(action), \"Acción inválida\"\n",
    "        done = False\n",
    "        truncated = False\n",
    "        self.action_last = action\n",
    "\n",
    "        # Realizar movimientos según la acción\n",
    "        if action == 0:  # Mover izquierda\n",
    "            self.robot_x_position = max(0, self.robot_x_position - 1)\n",
    "        elif action == 1:  # Mover derecha\n",
    "            self.robot_x_position = min(9, self.robot_x_position + 1)\n",
    "        elif action == 2:  # Mover abajo\n",
    "            self.robot_y_position = max(0, self.robot_y_position - 1)\n",
    "        elif action == 3:  # Mover arriba\n",
    "            self.robot_y_position = min(9, self.robot_y_position + 1)\n",
    "        elif action == 4:  # Mover atrás\n",
    "            self.robot_z_position = max(0, self.robot_z_position - 1)\n",
    "        elif action == 5:  # Mover adelante\n",
    "            self.robot_z_position = min(9, self.robot_z_position + 1)\n",
    "        elif action == 6:  # Abrir/cerrar pinza\n",
    "            if not self.has_piece:\n",
    "                if (self.robot_x_position == self.piece_x_position and\n",
    "                    self.robot_y_position == self.piece_y_position and\n",
    "                    self.robot_z_position == self.piece_z_position):\n",
    "                    self.has_piece = 1\n",
    "                    self.piece_x_position = -1\n",
    "                    self.piece_y_position = -1\n",
    "                    self.piece_z_position = -1\n",
    "            else:\n",
    "                self.has_piece = 0\n",
    "                self.piece_x_position = self.robot_x_position\n",
    "                self.piece_y_position = self.robot_y_position\n",
    "                self.piece_z_position = self.robot_z_position\n",
    "                if (self.piece_x_position == self.goal_x_position and\n",
    "                    self.piece_y_position == self.goal_y_position and\n",
    "                    self.piece_z_position == self.goal_z_position):\n",
    "                    done = True\n",
    "\n",
    "        # Calcular recompensa y verificar si se ha alcanzado el objetivo\n",
    "        reward = self.reward_function(self)\n",
    "        self.steps += 1\n",
    "        \n",
    "        if self.steps >= 1000:  # Límite de pasos por episodio\n",
    "            truncated = True\n",
    "        return self._get_observation(), reward, done, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renderiza el estado actual del entorno en una gráfica 3D.\n",
    "        \"\"\"\n",
    "        # Inicializar figura y ejes si no existen\n",
    "        if self.fig is None or self.ax is None:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "            # Inicializar gráficos del robot, la pieza y el objetivo\n",
    "            self.robot_plot, = self.ax.plot([], [], [], \"go\", label=\"Robot\")  # Punto verde para el robot\n",
    "            self.piece_plot, = self.ax.plot([], [], [], \"bo\", label=\"Pieza\")  # Punto azul para la pieza\n",
    "            self.goal_plot, = self.ax.plot([], [], [], \"ro\", label=\"Objetivo\")  # Punto rojo para el objetivo\n",
    "\n",
    "            # Configuración de los límites del eje (ajústalos según tu entorno)\n",
    "            self.ax.set_xlim([0, 10])\n",
    "            self.ax.set_ylim([0, 10])\n",
    "            self.ax.set_zlim([0, 10])\n",
    "\n",
    "            # Etiquetas y leyenda\n",
    "            self.ax.set_xlabel(\"X\")\n",
    "            self.ax.set_ylabel(\"Y\")\n",
    "            self.ax.set_zlabel(\"Z\")\n",
    "            self.ax.legend()\n",
    "\n",
    "        # Actualizar posición del robot\n",
    "        self.robot_plot.set_data([self.robot_x_position], [self.robot_y_position])\n",
    "        self.robot_plot.set_3d_properties([self.robot_z_position])\n",
    "\n",
    "        # Actualizar posición de la pieza\n",
    "        if self.has_piece:\n",
    "            self.piece_plot.set_data([self.robot_x_position], [self.robot_y_position])\n",
    "            self.piece_plot.set_3d_properties([self.robot_z_position])\n",
    "        else:\n",
    "            self.piece_plot.set_data([self.piece_x_position], [self.piece_y_position])\n",
    "            self.piece_plot.set_3d_properties([self.piece_z_position])\n",
    "\n",
    "        # Actualizar posición del objetivo\n",
    "        self.goal_plot.set_data([self.goal_x_position], [self.goal_y_position])\n",
    "        self.goal_plot.set_3d_properties([self.goal_z_position])\n",
    "\n",
    "        # Dibujar y pausar para visualización en tiempo real\n",
    "        plt.draw()\n",
    "        plt.pause(0.01)\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Obtiene la observación actual del entorno.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Estado actual del entorno.\n",
    "        \"\"\"\n",
    "        return (self.robot_x_position, self.robot_y_position, self.robot_z_position, self.has_piece, self.piece_x_position, self.piece_y_position, self.piece_z_position)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Cierra la visualización del entorno.\n",
    "        \"\"\"\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemetación de Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo with Exploring Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "id": "9388878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_es(env, num_episodes, gamma=0.9, eval_interval=1000, graph=False):\n",
    "    \"\"\"\n",
    "    Implementa el método de Monte Carlo con Exploring Starts para resolver el entorno del robot.\n",
    "\n",
    "    Args:\n",
    "        env: Entorno del robot.\n",
    "        num_episodes: Número de episodios a simular.\n",
    "        gamma: Factor de descuento.\n",
    "        eval_interval: Episodios entre evaluaciones de la política.\n",
    "        graph: Booleano para mostrar la gráfica de recompensas acumuladas.\n",
    "\n",
    "    Returns:\n",
    "        Q: Función de valor óptima.\n",
    "        policy: Política óptima.\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))  # Inicializar Q\n",
    "    returns = defaultdict(list)  # Retornos acumulados\n",
    "    eval_results = {}  # Diccionario para almacenar métricas de evaluación\n",
    "    accumulated_rewards = []  # Lista para almacenar recompensas acumuladas por episodio\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Paso 1: Exploring Starts - Estado y acción aleatorios\n",
    "        state = env.reset()[0]\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Paso 2: Generar un episodio completo\n",
    "        episode_data = []\n",
    "        done = False\n",
    "        truncated = False\n",
    "\n",
    "        while not (done or truncated):\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            episode_data.append((state, action, reward))\n",
    "            state = next_state\n",
    "            action = env.action_space.sample()  # Exploración aleatoria\n",
    "\n",
    "        # Paso 3: Calcular los retornos para cada par (estado, acción)\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode_data):\n",
    "            G = reward + gamma * G\n",
    "            if (state, action) not in visited:\n",
    "                returns[(state, action)].append(G)\n",
    "                Q[state][action] = np.mean(returns[(state, action)])\n",
    "                visited.add((state, action))\n",
    "\n",
    "        # Almacenar recompensa acumulada para este episodio\n",
    "        accumulated_rewards.append(G)\n",
    "\n",
    "        # Evaluar la política cada 'eval_interval' episodios\n",
    "        if (episode + 1) % eval_interval == 0:\n",
    "            policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "            eval_results = evaluate_policy_env(env, policy, num_episodes=10, episode=episode + 1, eval_results=eval_results)\n",
    "\n",
    "    if graph:\n",
    "        # Generar la gráfica de episodios vs recompensa acumulada y mostrarla\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(num_episodes), accumulated_rewards, label=\"Recompensa Acumulada\")\n",
    "        plt.xlabel(\"Episodios\")\n",
    "        plt.ylabel(\"Recompensa Acumulada\")\n",
    "        plt.title(f\"Episodios vs Recompensa Acumulada (Monte Carlo) ({env.reward_function.__name__})\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.savefig(f\"/Users/maria/Desktop/Máster IA/Aprendizaje por Refuerzo/Clase 2/Ejercicio 2/monte_carlo_accumulated_rewards_{env.reward_function.__name__}.png\")\n",
    "        plt.show()\n",
    "\n",
    "    # Derivar la política óptima de Q\n",
    "    policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "\n",
    "    # Mostrar gráficas finales de evaluación\n",
    "    print(\"\\nMostrando gráficas finales de evaluación...\")\n",
    "    plot_evaluation_results(eval_results)\n",
    "\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis del algoritmo Monte Carlo con Exploring Starts (ES)\n",
    "\n",
    "1. **Exploración (Exploring Starts)**\n",
    "- En cada episodio, el robot comienza desde un estado y una acción seleccionados de forma aleatoria. Asegura que todas las combinaciones de estados y acciones sean exploradas en algún momento, incluso en problemas con espacios de estados grandes o complejos.\n",
    "\n",
    "2. **Aprendizaje (retornos acumulados)**\n",
    "- Los valores `Q` se ajustan calculando el promedio de todos los retornos acumulados observados para cada par (estado, acción).\n",
    "\n",
    "3. **Derivación de la política**\n",
    "- Una vez calculados los valores `Q`, se deriva una política determinista seleccionando la acción con el mayor valor `Q` en cada estado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha=0.1, gamma=0.9, epsilon=1.0, eval_interval=1000, graph=False):\n",
    "    \"\"\"\n",
    "    Implementación del algoritmo Q-Learning.\n",
    "\n",
    "    Args:\n",
    "        env: Entorno del robot.\n",
    "        num_episodes: Número de episodios a simular.\n",
    "        alpha: Tasa de aprendizaje.\n",
    "        gamma: Factor de descuento.\n",
    "        epsilon: Tasa de exploración inicial.\n",
    "        eval_interval: Episodios entre evaluaciones de la política.\n",
    "        graph: Booleano para mostrar la gráfica de recompensas acumuladas.\n",
    "\n",
    "    Returns:\n",
    "        Q: Función de valor óptima.\n",
    "        policy: Política óptima.\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    eval_results = {}  # Diccionario para acumular evaluaciones\n",
    "    accumulated_rewards = []  # Lista para almacenar recompensas acumuladas por episodio\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        epsilon_end = 0.05\n",
    "        epsilon_dec = max(epsilon_end, epsilon - (epsilon - epsilon_end) * episode / num_episodes) # Decae gradualmente el valor de epsilon para reducir la exploración a medida que el agente aprende.\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not (done or truncated):\n",
    "            # Selección de acción según la estrategia epsilon-greedy.\n",
    "            if np.random.rand() < epsilon_dec:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            \n",
    "            # Actualización de la función Q usando la ecuación de Q-Learning.\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state][best_next_action] - Q[state][action])\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        # Almacenar recompensa acumulada para este episodio\n",
    "        accumulated_rewards.append(episode_reward)\n",
    "\n",
    "        # Evaluar la política cada 'eval_interval' episodios\n",
    "        if (episode + 1) % eval_interval == 0:\n",
    "            print(f\"\\nEvaluación tras el episodio {episode + 1}:\")\n",
    "            policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "            eval_results = evaluate_policy_env(env, policy, num_episodes=10, episode=episode + 1, eval_results=eval_results)\n",
    "\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            print(f\"Episodio {episode + 1}/{num_episodes} completado\")\n",
    "\n",
    "    if graph:\n",
    "    # Generar la gráfica de episodios vs recompensa acumulada y mostrarla\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(num_episodes), accumulated_rewards, label=\"Recompensa Acumulada\")\n",
    "        plt.xlabel(\"Episodios\")\n",
    "        plt.ylabel(\"Recompensa Acumulada\")\n",
    "        plt.title(f\"Episodios vs Recompensa Acumulada (Q-Learning) ({env.reward_function.__name__})\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.savefig(f\"/Users/maria/Desktop/Máster IA/Aprendizaje por Refuerzo/Clase 2/Ejercicio 2/q_learning_accumulated_rewards_{env.reward_function.__name__}.png\")\n",
    "        plt.show()\n",
    "\n",
    "    # Derivar la política óptima de Q\n",
    "    policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "\n",
    "    # Mostrar gráficas finales de evaluación\n",
    "    print(\"\\nMostrando gráficas finales de evaluación...\")\n",
    "    plot_evaluation_results(eval_results)\n",
    "\n",
    "    return Q, policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis detallado del algoritmo Q-Learning:\n",
    "1. **Exploración (epsilon-greedy)**:\n",
    "- En cada paso, el robot tiene una probabilidad epsilon de explorar tomando una acción aleatoria.\n",
    "- En el resto de los casos, el robot explota su conocimiento seleccionando la acción con el mayor valor Q.\n",
    "\n",
    "2. **Aprendizaje (actualización con alpha)**:\n",
    "- En la actualización de Q utiliza una tasa de aprendizaje (alpha) para ajustar gradualmente los valores Q hacia los retornos esperados. Así, las acciones con mayores recompensas futuras se valoren más.\n",
    "\n",
    "3. **Derivación de la política**:\n",
    "- Después del entrenamiento, la política óptima se construye seleccionando la acción con el mayor valor Q en cada estado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Política de Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_env(env, policy, num_episodes=100, episode=None, eval_results={}):\n",
    "    \"\"\"\n",
    "    Calcula las métricas de evaluación de una política en el entorno.\n",
    "\n",
    "    Args:\n",
    "        env: Entorno de simulación.\n",
    "        policy (dict): Política a evaluar.\n",
    "        num_episodes (int): Número de episodios para la evaluación.\n",
    "        episode (int): Número de episodio actual (opcional, para referencia).\n",
    "        eval_results (dict): Diccionario para acumular resultados de evaluaciones (opcional).\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con métricas acumuladas si eval_results es proporcionado.\n",
    "    \"\"\"\n",
    "    rewards = np.zeros(num_episodes)\n",
    "    goals_reached = 0\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not (done or truncated):\n",
    "            action = policy.get(state, env.action_space.sample())\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards[i] = total_reward\n",
    "        if done:\n",
    "            goals_reached += 1\n",
    "\n",
    "    # Calcular métricas principales\n",
    "    average_reward = np.mean(rewards)\n",
    "    goal_success_rate = (goals_reached / num_episodes) * 100\n",
    "\n",
    "    # Mostrar métricas en tiempo real\n",
    "    if episode is not None:\n",
    "        print(f\"[Episodio {episode}] Recompensa promedio: {average_reward:.2f}\")\n",
    "        print(f\"[Episodio {episode}] Porcentaje de episodios exitosos: {goal_success_rate:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Recompensa promedio: {average_reward:.2f}\")\n",
    "        print(f\"Porcentaje de episodios exitosos: {goal_success_rate:.2f}%\")\n",
    "\n",
    "    # Acumular resultados si eval_results se proporciona\n",
    "    if eval_results is not None:\n",
    "        eval_results.setdefault('episodes', []).append(episode or len(eval_results.get('episodes', [])))\n",
    "        eval_results.setdefault('average_rewards', []).append(average_reward)\n",
    "        eval_results.setdefault('success_rates', []).append(goal_success_rate)\n",
    "\n",
    "    return eval_results\n",
    "\n",
    "def plot_evaluation_results(eval_results):\n",
    "    \"\"\"\n",
    "    Genera las gráficas de las métricas acumuladas durante el entrenamiento.\n",
    "\n",
    "    Args:\n",
    "        eval_results (dict): Resultados acumulados de evaluaciones.\n",
    "    \"\"\"\n",
    "    episodes = eval_results['episodes']\n",
    "    average_rewards = eval_results['average_rewards']\n",
    "    success_rates = eval_results['success_rates']\n",
    "\n",
    "    print(\"\\nResultados finales de evaluación:\")\n",
    "    for ep, reward, success in zip(episodes, average_rewards, success_rates):\n",
    "        print(f\"Episodio {ep}: Recompensa promedio = {reward:.2f}, Tasa de éxito = {success:.2f}%\")\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Gráfica de recompensas promedio\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episodes, average_rewards, marker='o', linestyle='-')\n",
    "    plt.title(\"Recompensas promedio por evaluación\")\n",
    "    plt.xlabel(\"Episodio de evaluación\")\n",
    "    plt.ylabel(\"Recompensa promedio\")\n",
    "\n",
    "    # Gráfica de tasas de éxito\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(episodes, success_rates, marker='o', linestyle='-')\n",
    "    plt.title(\"Tasa de éxito por evaluación\")\n",
    "    plt.xlabel(\"Episodio de evaluación\")\n",
    "    plt.ylabel(\"Tasa de éxito (%)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot Pick and Place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renderizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_optimal_policy(env, policy, max_steps=100, pause_time=0.5):\n",
    "    \"\"\"\n",
    "    Renderiza paso a paso la ejecución de la política óptima en el entorno.\n",
    "\n",
    "    Args:\n",
    "        env: El entorno Gymnasium en el que se ejecutará la política.\n",
    "        policy (dict): La política óptima, donde las claves son estados y los valores son acciones.\n",
    "        max_steps (int): Número máximo de pasos para ejecutar el episodio.\n",
    "        pause_time (float): Tiempo de pausa entre pasos para visualización (en segundos).\n",
    "\n",
    "    Returns:\n",
    "        dict: Resultados del episodio, incluyendo recompensa total y si el objetivo fue alcanzado.\n",
    "    \"\"\"\n",
    "    # Reinicia el entorno y obtiene el estado inicial.\n",
    "    state = env.reset()[0]\n",
    "    env.render()\n",
    "\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    goal_reached = False\n",
    "    done = False\n",
    "    truncated = False\n",
    "\n",
    "    while steps < max_steps:\n",
    "        # Obtiene la acción de la política para el estado actual.\n",
    "        action = policy.get(state, env.action_space.sample())\n",
    "        print(f\"Estado: {state}, Acción seleccionada: {policy.get(state, 'Acción aleatoria')}\")\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        # Renderiza el entorno después de realizar la acción.\n",
    "        env.render()\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        plt.pause(pause_time)\n",
    "\n",
    "    env.close()\n",
    "    plt.close()\n",
    "\n",
    "    results = {\n",
    "        \"total_reward\": total_reward,\n",
    "        \"steps\": steps,\n",
    "        \"goal_reached\": done\n",
    "    }\n",
    "    \n",
    "    # Muestra un resumen de los resultados del episodio.\n",
    "    print(f\"Episodio terminado en {steps} pasos.\")\n",
    "    print(f\"Recompensa total: {total_reward:.2f}\")\n",
    "    print(f\"Objetivo alcanzado: {'Sí' if done else 'No'}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegir la función de recompensa a utilizar: calculate_reward_simple, calculate_reward_distance o calculate_reward_mixed\n",
    "env = RobotPickAndPlaceEnv(reward_function=calculate_reward_simple)\n",
    "num_episodes = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Monte Carlo...\n",
      "[Episodio 1000] Recompensa promedio: -11674047900.00\n",
      "[Episodio 1000] Porcentaje de episodios exitosos: 0.00%\n",
      "[Episodio 2000] Recompensa promedio: -4027500000.00\n",
      "[Episodio 2000] Porcentaje de episodios exitosos: 0.00%\n",
      "[Episodio 3000] Recompensa promedio: -5143000000.00\n",
      "[Episodio 3000] Porcentaje de episodios exitosos: 0.00%\n",
      "\n",
      "Mostrando gráficas finales de evaluación...\n",
      "\n",
      "Resultados finales de evaluación:\n",
      "Episodio 1000: Recompensa promedio = -11674047900.00, Tasa de éxito = 0.00%\n",
      "Episodio 2000: Recompensa promedio = -4027500000.00, Tasa de éxito = 0.00%\n",
      "Episodio 3000: Recompensa promedio = -5143000000.00, Tasa de éxito = 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Entrenando Monte Carlo...\")\n",
    "Q_mc, policy_mc = monte_carlo_es(env, num_episodes, graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizando la política óptima...\n",
      "Estado: 0, Acción seleccionada: 5\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 0\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Episodio terminado en 100 pasos.\n",
      "Recompensa total: -766800000.00\n",
      "Objetivo alcanzado: No\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_reward': -766800000, 'steps': 100, 'goal_reached': False}"
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Visualizando la política óptima...\")\n",
    "render_optimal_policy(env, policy_mc, max_steps=100, pause_time=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegir la función de recompensa a utilizar: calculate_reward_simple, calculate_reward_distance o calculate_reward_mixed\n",
    "env = RobotPickAndPlaceEnv(reward_function=calculate_reward_simple)\n",
    "num_episodes = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Q-Learning...\n",
      "\n",
      "Evaluación tras el episodio 1000:\n",
      "[Episodio 1000] Recompensa promedio: -2953000000.00\n",
      "[Episodio 1000] Porcentaje de episodios exitosos: 0.00%\n",
      "Episodio 1000/3000 completado\n",
      "\n",
      "Evaluación tras el episodio 2000:\n",
      "[Episodio 2000] Recompensa promedio: -920470.00\n",
      "[Episodio 2000] Porcentaje de episodios exitosos: 0.00%\n",
      "Episodio 2000/3000 completado\n",
      "\n",
      "Evaluación tras el episodio 3000:\n",
      "[Episodio 3000] Recompensa promedio: -102590.00\n",
      "[Episodio 3000] Porcentaje de episodios exitosos: 100.00%\n",
      "Episodio 3000/3000 completado\n",
      "\n",
      "Mostrando gráficas finales de evaluación...\n",
      "\n",
      "Resultados finales de evaluación:\n",
      "Episodio 1000: Recompensa promedio = -2953000000.00, Tasa de éxito = 0.00%\n",
      "Episodio 2000: Recompensa promedio = -920470.00, Tasa de éxito = 0.00%\n",
      "Episodio 3000: Recompensa promedio = -102590.00, Tasa de éxito = 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Entrenando Q-Learning...\")\n",
    "Q_ql, policy_ql = q_learning(env, num_episodes, alpha=0.1, gamma=0.9, epsilon=1.0, graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizando la política óptima...\n",
      "Estado: 0, Acción seleccionada: 5\n",
      "Estado: (0, 0, 1, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (1, 0, 1, 0, 5, 5, 5), Acción seleccionada: 3\n",
      "Estado: (1, 1, 1, 0, 5, 5, 5), Acción seleccionada: 3\n",
      "Estado: (1, 2, 1, 0, 5, 5, 5), Acción seleccionada: 5\n",
      "Estado: (1, 2, 2, 0, 5, 5, 5), Acción seleccionada: 5\n",
      "Estado: (1, 2, 3, 0, 5, 5, 5), Acción seleccionada: 3\n",
      "Estado: (1, 3, 3, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (2, 3, 3, 0, 5, 5, 5), Acción seleccionada: 3\n",
      "Estado: (2, 4, 3, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (3, 4, 3, 0, 5, 5, 5), Acción seleccionada: 5\n",
      "Estado: (3, 4, 4, 0, 5, 5, 5), Acción seleccionada: 3\n",
      "Estado: (3, 5, 4, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (4, 5, 4, 0, 5, 5, 5), Acción seleccionada: 1\n",
      "Estado: (5, 5, 4, 0, 5, 5, 5), Acción seleccionada: 5\n",
      "Estado: (5, 5, 5, 0, 5, 5, 5), Acción seleccionada: 6\n",
      "Estado: (5, 5, 5, 1, -1, -1, -1), Acción seleccionada: 3\n",
      "Estado: (5, 6, 5, 1, -1, -1, -1), Acción seleccionada: 3\n",
      "Estado: (5, 7, 5, 1, -1, -1, -1), Acción seleccionada: 5\n",
      "Estado: (5, 7, 6, 1, -1, -1, -1), Acción seleccionada: 5\n",
      "Estado: (5, 7, 7, 1, -1, -1, -1), Acción seleccionada: 5\n",
      "Estado: (5, 7, 8, 1, -1, -1, -1), Acción seleccionada: 5\n",
      "Estado: (5, 7, 9, 1, -1, -1, -1), Acción seleccionada: 1\n",
      "Estado: (6, 7, 9, 1, -1, -1, -1), Acción seleccionada: 1\n",
      "Estado: (7, 7, 9, 1, -1, -1, -1), Acción seleccionada: 1\n",
      "Estado: (8, 7, 9, 1, -1, -1, -1), Acción seleccionada: 1\n",
      "Estado: (9, 7, 9, 1, -1, -1, -1), Acción seleccionada: 6\n",
      "Episodio terminado en 27 pasos.\n",
      "Recompensa total: -57680.00\n",
      "Objetivo alcanzado: Sí\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_reward': -57680, 'steps': 27, 'goal_reached': True}"
      ]
     },
     "execution_count": 894,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Visualizando la política óptima...\")\n",
    "render_optimal_policy(env, policy_ql, max_steps=100, pause_time=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Monte Carlo en Frozen Lake...\n",
      "[Episodio 1000] Recompensa promedio: 1.00\n",
      "[Episodio 1000] Porcentaje de episodios exitosos: 100.00%\n",
      "[Episodio 2000] Recompensa promedio: 1.00\n",
      "[Episodio 2000] Porcentaje de episodios exitosos: 100.00%\n",
      "[Episodio 3000] Recompensa promedio: 1.00\n",
      "[Episodio 3000] Porcentaje de episodios exitosos: 100.00%\n",
      "[Episodio 4000] Recompensa promedio: 1.00\n",
      "[Episodio 4000] Porcentaje de episodios exitosos: 100.00%\n",
      "[Episodio 5000] Recompensa promedio: 1.00\n",
      "[Episodio 5000] Porcentaje de episodios exitosos: 100.00%\n",
      "\n",
      "Mostrando gráficas finales de evaluación...\n",
      "\n",
      "Resultados finales de evaluación:\n",
      "Episodio 1000: Recompensa promedio = 1.00, Tasa de éxito = 100.00%\n",
      "Episodio 2000: Recompensa promedio = 1.00, Tasa de éxito = 100.00%\n",
      "Episodio 3000: Recompensa promedio = 1.00, Tasa de éxito = 100.00%\n",
      "Episodio 4000: Recompensa promedio = 1.00, Tasa de éxito = 100.00%\n",
      "Episodio 5000: Recompensa promedio = 1.00, Tasa de éxito = 100.00%\n",
      "Política Monte Carlo en FrozenLake: {8: 2, 4: 1, 0: 1, 1: 2, 6: 1, 2: 1, 10: 1, 14: 2, 13: 2, 9: 1, 3: 0}\n",
      "Entrenando Q-Learning en Frozen Lake...\n",
      "\n",
      "Evaluación tras el episodio 1000:\n",
      "[Episodio 1000] Recompensa promedio: 1.00\n",
      "[Episodio 1000] Porcentaje de episodios exitosos: 100.00%\n",
      "Episodio 1000/5000 completado\n",
      "\n",
      "Evaluación tras el episodio 2000:\n",
      "[Episodio 2000] Recompensa promedio: 1.00\n",
      "[Episodio 2000] Porcentaje de episodios exitosos: 100.00%\n",
      "Episodio 2000/5000 completado\n",
      "\n",
      "Evaluación tras el episodio 3000:\n",
      "[Episodio 3000] Recompensa promedio: 1.00\n",
      "[Episodio 3000] Porcentaje de episodios exitosos: 100.00%\n",
      "Episodio 3000/5000 completado\n",
      "\n",
      "Evaluación tras el episodio 4000:\n",
      "[Episodio 4000] Recompensa promedio: 1.00\n",
      "[Episodio 4000] Porcentaje de episodios exitosos: 100.00%\n",
      "Episodio 4000/5000 completado\n",
      "\n",
      "Evaluación tras el episodio 5000:\n",
      "[Episodio 5000] Recompensa promedio: 1.00\n",
      "[Episodio 5000] Porcentaje de episodios exitosos: 100.00%\n",
      "Episodio 5000/5000 completado\n",
      "\n",
      "Mostrando gráficas finales de evaluación...\n",
      "\n",
      "Resultados finales de evaluación:\n",
      "Episodio 1000: Recompensa promedio = 1.00, Tasa de éxito = 100.00%\n",
      "Episodio 2000: Recompensa promedio = 1.00, Tasa de éxito = 100.00%\n",
      "Episodio 3000: Recompensa promedio = 1.00, Tasa de éxito = 100.00%\n",
      "Episodio 4000: Recompensa promedio = 1.00, Tasa de éxito = 100.00%\n",
      "Episodio 5000: Recompensa promedio = 1.00, Tasa de éxito = 100.00%\n",
      "Política Q-Learning en FrozenLake: {4: 1, 0: 1, 8: 2, 9: 2, 13: 2, 12: 0, 1: 2, 5: 0, 2: 1, 10: 1, 14: 2, 11: 0, 3: 0, 7: 0, 6: 1, 15: 0}\n"
     ]
    }
   ],
   "source": [
    "def run_gymnasium_example():\n",
    "    env = gymnasium.make('FrozenLake-v1', is_slippery=False)\n",
    "    num_episodes = 5000\n",
    "\n",
    "    print(\"Entrenando Monte Carlo en Frozen Lake...\")\n",
    "    Q_mc, policy_mc = monte_carlo_es(env, num_episodes)\n",
    "    print('Política Monte Carlo en FrozenLake:', policy_mc)\n",
    "\n",
    "    print(\"Entrenando Q-Learning en Frozen Lake...\")\n",
    "    Q_ql, policy_ql = q_learning(env, num_episodes, alpha=0.1, gamma=0.9, epsilon=1.0)\n",
    "    print('Política Q-Learning en FrozenLake:', policy_ql)\n",
    "\n",
    "# Probar Frozen Lake\n",
    "run_gymnasium_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ejecutados ambos algoritmos con las 3 funciones de recompensa, sacamos las siguientes conclusiones:\n",
    "\n",
    "- El algoritmo de Monte Carlo no funciona correctamente con niguna de las funciones. No es un algoritmo que funcione  bien en entornos complejos y con funciones de recompensa poco claras.\n",
    "\n",
    "- Q-Learning converge y cumple con el objetivo usando las 3 funciones. Muestra su adaptabilidad en entornos más complejos gracias a la actualización paso a paso y su capacidad para trabajar con recompensas intermedias.\n",
    "\n",
    "- Ambos algoritmos funcionan bien en el entorno de Frozen Lake, por lo que confirmamos que son efectivos en problemas discretos más simples y recompensas bien definidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate_reward_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la función simple, el robot practicamente no se mueve del punto de origen y entra en bucle. Esto puede ser porque el algoritmo da una política exploratoria inicial y la función de recompensa no incita al robot a explorar en dirección hacia la pieza con recompensas intermedias suficientes.\n",
    "\n",
    "De la gráfica podemos decir que el robot no aprende nada y está estancado en recompensas negativas. No hay indicios de posible mejora en una ejecución con mayor número de episodios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Recompensa acumulada en Monte Carlo con la función de recompensa simple](./monte_carlo_accumulated_rewards_calculate_reward_simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con Q-Learning, la función simple entrena al robot para llegar al objetivo. Si observamos la gráfica de recompensa acumulada vemos que:\n",
    "\n",
    "- La recompensa acumulada empieza con valores negativos, lo que indica exploración inicial y penalizaciones. También hay variaciones, estas pueden ser por la tasa de exploración.\n",
    "\n",
    "- Las variaciones van disminuyendo y se empiezan a estabilizar, lo que indica que el robot está aprendiendo una buena política basada en las recompensas obtenidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Recompensa acumulada en Q-Learning con la función de recompensa simple](./q_learning_accumulated_rewards_calculate_reward_simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate_reward_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta función, Monte Carlo no funciona bien ya que, después de coger la pieza, entra en un bucle. Esto puede deberse a que el algoritmo en sí funciona con una política inicial aleatoria y, si nada le hace salir del bucle, se queda ahí. También, es posible que el mal rendimiento que proporciona sea porque limitamos el número de pasos a 1000, por falta de capacidad de computación, y no aprende lo suficiente.\n",
    "\n",
    "Observamos en la gráfica que, sin embargo, la recompensa acumulada por episodio es positiva y alta. Esto se debe a que con esta función no penalizamos y la recompensa es siempre proporcional a la distancia del objetivo, ya sea coger la pieza o soltarla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Recompensa acumulada en Monte Carlo con la función de recompensa de distancia](./monte_carlo_accumulated_rewards_calculate_reward_distance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la función de recompensa basada en la distancia, Q-Learning converge y suelta la pieza en el objetivo. Observamos como evoluciona la recompensa acumulada a lo largo de los episodios de entrenamiento:\n",
    "\n",
    "- Durante la primera mitad de los episodios, la recompensa acumulada muestra una tendencia creciente con variabilidad, indicando que el robot está aprendiendo a realizar acciones más efectivas, logrando acercarse al objetivo y completar la tarea con mayor frecuencia.\n",
    "\n",
    "- En los isguientes episodios, la recompensa acumulada se empieza a estabilizar, lo que sugiere que el robot ha aprendido una política buena y está ejecutando las acciones necesarias para cumplir el objetivo de manera eficiente.\n",
    "\n",
    "- Hacia el final de los episodios, hay cierta variabilidad en las recompensas, puede deberse a que el robot siga explorando posibles rutas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Recompensa acumulada en Q-Learning con la función de recompensa de distancia](./q_learning_accumulated_rewards_calculate_reward_distance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate_reward_mixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la función mixed, Monte Carlo no consigue que el robot llegue a coger la pieza. Mirando la gráfica, observamos como las recompensas son bastante negativas, lo que nos deja ver que la mayoría de acciones realizadas por el robot han sido penalizadas.\n",
    "\n",
    "Tampoco podemos observar que haya indicios de aprendizaje ya que, aunque hay variaciones, el valor de la recompensa acumulada es bastante constante, no hay tendencia positiva.\n",
    "\n",
    "Estos resultados pueden deberse a que Monte Carlo depende de la exploración para encontrar políticas efectivas y si la función no es buena, los intentos aleatorios iniciales no logran guiar al robot y se puede quedar atrapado en una política mala. Además, Monte Carlo suele necesitar más episodios para converger que Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Recompensa acumulada en Monte Carlo con la función de recompensa mixed](./monte_carlo_accumulated_rewards_calculate_reward_mixed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta función, Q-Learning entrena al robot y consigue llegar al objetivo. Analizando la gráfica vemos:\n",
    "\n",
    "- Al principio, las recompensas acumuladas son negativas, probablemente porque la función de recompensa aplica penalizaciones altas por errores y movimientos redundantes.\n",
    "\n",
    "- Durante el segundo período, entre los 1000 y 2000 episodios, las recompensas acumuladas comienzan a aumentar. Parece que el robot comienza a encontrar estrategias más efectivas para minimizar las penalizaciones y maximizar las recompensas.\n",
    "\n",
    "- En los últimos episodios, las recompensas se estabilizan, con algunas variaciones. Esto indica que el robot ha aprendido una política buena para cumplir el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Recompensa acumulada en Q-Learning con la función de recompensa mixed](./q_learning_accumulated_rewards_calculate_reward_mixed.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
